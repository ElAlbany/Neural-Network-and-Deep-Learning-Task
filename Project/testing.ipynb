{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e282de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8d9365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading augmented data from train_augmented.csv...\n"
     ]
    }
   ],
   "source": [
    "df = main.load_data(\"train_augmented.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea6bc387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8,000\n"
     ]
    }
   ],
   "source": [
    "vocab = main.create_optimal_vocabulary(df[\"text\"].tolist(), target_size=8000)\n",
    "print(f\"Vocabulary size: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e59199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 12,211 texts with max_len=150...\n"
     ]
    }
   ],
   "source": [
    "X, y, label_map = main.encode_dataset(df, vocab, max_len=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138e110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoaders created:\n",
      "  Train: 8,547 samples, 134 batches\n",
      "  Val:   1,832 samples, 29 batches\n",
      "  Test:  1,832 samples, 29 batches\n",
      "Using device: cpu\n",
      "Starting training for 10 epochs on cpu...\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.4625, Train Acc: 32.68%\n",
      "  Val   Loss: 1.1143, Val   Acc: 51.75%\n",
      "------------------------------------------------------------\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.9843, Train Acc: 59.81%\n",
      "  Val   Loss: 0.8556, Val   Acc: 64.79%\n",
      "------------------------------------------------------------\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.6942, Train Acc: 73.51%\n",
      "  Val   Loss: 0.7326, Val   Acc: 69.65%\n",
      "------------------------------------------------------------\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.5252, Train Acc: 80.33%\n",
      "  Val   Loss: 0.6651, Val   Acc: 73.42%\n",
      "------------------------------------------------------------\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.4072, Train Acc: 85.47%\n",
      "  Val   Loss: 0.6903, Val   Acc: 73.42%\n",
      "------------------------------------------------------------\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.3222, Train Acc: 88.76%\n",
      "  Val   Loss: 0.7051, Val   Acc: 75.22%\n",
      "------------------------------------------------------------\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.2619, Train Acc: 91.01%\n",
      "  Val   Loss: 0.7674, Val   Acc: 74.40%\n",
      "------------------------------------------------------------\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.2174, Train Acc: 93.03%\n",
      "  Val   Loss: 0.8729, Val   Acc: 75.11%\n",
      "------------------------------------------------------------\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.1805, Train Acc: 94.65%\n",
      "  Val   Loss: 0.9767, Val   Acc: 74.29%\n",
      "------------------------------------------------------------\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.1565, Train Acc: 95.32%\n",
      "  Val   Loss: 0.9886, Val   Acc: 75.11%\n",
      "------------------------------------------------------------\n",
      "‚úÖ Loaded best validation state (Val Acc: 75.22%)\n"
     ]
    }
   ],
   "source": [
    "# Make loaders\n",
    "train_loader, val_loader, test_loader = main.make_loaders(X, y, batch_size=64)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = main.TextCNNGLU(vocab_size=len(vocab), embed_dim=128, num_classes=len(label_map)).to(device)\n",
    "\n",
    "# Train (in-memory best model)\n",
    "model = main.train_textcnn(model, train_loader, val_loader, device, num_epochs=10, lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65ee15b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST SET RESULTS (TextCNNGLU)\n",
      "============================================================\n",
      "Loss: 0.9532\n",
      "Accuracy: 74.67%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Very bad       0.96      0.95      0.95       371\n",
      "         Bad       0.85      0.94      0.89       370\n",
      "        Good       0.78      0.80      0.79       371\n",
      "   Very good       0.51      0.52      0.52       370\n",
      "   Excellent       0.60      0.51      0.55       350\n",
      "\n",
      "    accuracy                           0.75      1832\n",
      "   macro avg       0.74      0.74      0.74      1832\n",
      "weighted avg       0.74      0.75      0.74      1832\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[353  10   5   1   2]\n",
      " [  5 348  10   3   4]\n",
      " [  2  22 297  40  10]\n",
      " [  6  20  50 193 101]\n",
      " [  3   9  19 142 177]]\n"
     ]
    }
   ],
   "source": [
    "main.evaluate(model, test_loader, device, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72e543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddb978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a911d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6a6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNNGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with Gated Linear Units for better performance on small datasets\n",
    "    Combines the efficiency of CNNs with the gating mechanism of LSTMs\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_classes=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. EMBEDDING LAYER\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embed_dropout = nn.Dropout(0.5)  # Heavy dropout for small data\n",
    "        \n",
    "        # 2. CONVOLUTIONAL BLOCKS WITH GLU\n",
    "        # Multiple filter sizes capture different n-gram patterns\n",
    "        self.conv3 = nn.Conv1d(embed_dim, 64, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(embed_dim, 64, kernel_size=5, padding=2)\n",
    "        self.conv7 = nn.Conv1d(embed_dim, 64, kernel_size=7, padding=3)\n",
    "        \n",
    "        # 3. GATED LINEAR UNITS (alternative to ReLU)\n",
    "        # GLU(x) = x * sigmoid(x)  - helps with gradient flow\n",
    "        self.glu = nn.GLU(dim=1)  # Will reduce channels by half\n",
    "        \n",
    "        # 4. ATTENTION POOLING (instead of simple max/avg)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(96, 64),  # 96 = 64*3/2 (after GLU reduction)\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1, bias=False)\n",
    "        )\n",
    "        \n",
    "        # 5. CLASSIFIER WITH HEAVY REGULARIZATION\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(96, 48),\n",
    "            nn.LayerNorm(48),  # LayerNorm better than BatchNorm for small batches\n",
    "            nn.GELU(),  # GELU better than ReLU for small data\n",
    "            nn.Dropout(0.6),  # Very high dropout\n",
    "            nn.Linear(48, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 6. INITIALIZATION CRUCIAL FOR SMALL DATA\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Proper initialization is critical for small datasets\"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_out', nonlinearity='linear')\n",
    "        nn.init.kaiming_normal_(self.conv5.weight, mode='fan_out', nonlinearity='linear')\n",
    "        nn.init.kaiming_normal_(self.conv7.weight, mode='fan_out', nonlinearity='linear')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        x = self.embed_dropout(x)\n",
    "        \n",
    "        # Transpose for conv1d\n",
    "        x = x.transpose(1, 2)  # (batch, embed_dim, seq_len)\n",
    "        \n",
    "        # Apply different convolutional filters\n",
    "        conv3_out = self.conv3(x)\n",
    "        conv5_out = self.conv5(x)\n",
    "        conv7_out = self.conv7(x)\n",
    "        \n",
    "        # Apply GLU for gating\n",
    "        conv3_out = self.glu(conv3_out)  # (batch, 32, seq_len)\n",
    "        conv5_out = self.glu(conv5_out)  # (batch, 32, seq_len)\n",
    "        conv7_out = self.glu(conv7_out)  # (batch, 32, seq_len)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        combined = torch.cat([conv3_out, conv5_out, conv7_out], dim=1)  # (batch, 96, seq_len)\n",
    "        combined = combined.transpose(1, 2)  # (batch, seq_len, 96)\n",
    "        \n",
    "        # Apply attention pooling\n",
    "        attn_weights = F.softmax(self.attention(combined), dim=1)  # (batch, seq_len, 1)\n",
    "        weighted = torch.sum(attn_weights * combined, dim=1)  # (batch, 96)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(weighted)\n",
    "        return output, attn_weights  # Return attention for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimal_vocabulary(texts, target_size=8000):\n",
    "    \"\"\"\n",
    "    Create vocabulary optimized for small datasets\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    \n",
    "    word_counts = Counter()\n",
    "    char_counts = Counter()\n",
    "    \n",
    "    for text in texts:\n",
    "        # Keep basic punctuation for CNN to learn patterns\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w[\\w\\'\\-]+\\b', text)  # Keep hyphenated, contractions\n",
    "        \n",
    "        word_counts.update(words)\n",
    "        \n",
    "        # Also count character 3-grams for subword info\n",
    "        for word in words:\n",
    "            for i in range(len(word)-2):\n",
    "                char_counts[word[i:i+3]] += 1\n",
    "    \n",
    "    # Strategy: Keep frequent words AND informative rare words\n",
    "    vocab = []\n",
    "    \n",
    "    # 1. Top 6000 most frequent words\n",
    "    top_words = [word for word, _ in word_counts.most_common(6000)]\n",
    "    vocab.extend(top_words)\n",
    "    \n",
    "    # 2. Add words with high TF-IDF-like scores (even if less frequent)\n",
    "    total_words = sum(word_counts.values())\n",
    "    avg_freq = total_words / len(word_counts)\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if 5 <= count <= 20:  # Mid-frequency range\n",
    "            # Calculate simple \"importance\" score\n",
    "            doc_freq = sum(1 for t in texts if word in t)\n",
    "            if doc_freq <= len(texts) * 0.1:  # Appears in <10% of documents\n",
    "                vocab.append(word)  # Likely informative\n",
    "    \n",
    "    # 3. Add common character n-grams for subword modeling\n",
    "    top_chars = [chars for chars, _ in char_counts.most_common(1000)]\n",
    "    vocab.extend([f\"##{chars}##\" for chars in top_chars])  # Mark as subword\n",
    "    \n",
    "    # 4. Limit to target size and add special tokens\n",
    "    vocab = list(set(vocab))[:target_size-3]\n",
    "    vocab = ['<PAD>', '<UNK>', '<NUM>'] + vocab\n",
    "    \n",
    "    return {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_for_small_data(text, vocab, max_len=150):\n",
    "    \"\"\"\n",
    "    Advanced preprocessing for limited data\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # 1. Conservative normalization\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Handle numbers specially (common in reviews)\n",
    "    text = re.sub(r'\\d+', ' <NUM> ', text)\n",
    "    \n",
    "    # 3. Keep useful punctuation for CNN patterns\n",
    "    # CNN can learn from !!! vs . vs ? patterns\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1', text)  # Reduce repeated punctuation\n",
    "    text = re.sub(r'([!?.])', r' \\1 ', text)     # Add spaces around punctuation\n",
    "    \n",
    "    # 4. Tokenize preserving some structure\n",
    "    tokens = []\n",
    "    for part in text.split():\n",
    "        if part in ['.', '!', '?', ',', ';', ':']:\n",
    "            tokens.append(part)  # Keep punctuation as separate tokens\n",
    "        else:\n",
    "            # Split into words, handling contractions\n",
    "            words = re.findall(r'\\b\\w[\\w\\'\\-]+\\b', part)\n",
    "            tokens.extend(words)\n",
    "    \n",
    "    # 5. Encode with subword fallback\n",
    "    encoded = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            encoded.append(vocab[token])\n",
    "        else:\n",
    "            # Try subword segmentation\n",
    "            found = False\n",
    "            for i in range(3, len(token)):\n",
    "                subword = f\"##{token[i-3:i]}##\"\n",
    "                if subword in vocab:\n",
    "                    encoded.append(vocab[subword])\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                encoded.append(vocab['<UNK>'])\n",
    "    \n",
    "    # 6. Dynamic padding/truncation\n",
    "    if len(encoded) > max_len:\n",
    "        # Keep beginning and end (most informative parts)\n",
    "        keep_start = encoded[:max_len//2]\n",
    "        keep_end = encoded[-(max_len//2):]\n",
    "        encoded = keep_start + keep_end\n",
    "    else:\n",
    "        encoded = encoded + [vocab['<PAD>']] * (max_len - len(encoded))\n",
    "    \n",
    "    return encoded[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_small_data_model(model, train_loader, val_loader, num_epochs=50):\n",
    "    \"\"\"\n",
    "    Specialized training for small datasets\n",
    "    \"\"\"\n",
    "    # 1. OPTIMIZER: AdamW with decoupled weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.01,  # Strong weight decay\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # 2. SCHEDULER: OneCycleLR for faster convergence\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1  # Short warmup\n",
    "    )\n",
    "    \n",
    "    # 3. LOSS: Label Smoothing + Class Weights\n",
    "    class_weights = calculate_class_weights(train_labels)\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        weight=class_weights,\n",
    "        label_smoothing=0.1  # Reduces overconfidence\n",
    "    )\n",
    "    \n",
    "    # 4. REGULARIZATION: Early Stopping with Plateau\n",
    "    early_stopping = EarlyStopping(patience=10, delta=0.001)\n",
    "    \n",
    "    # 5. TRAINING LOOP\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Use MixUp augmentation (even for text)\n",
    "        for batch in train_loader:\n",
    "            texts, labels = batch\n",
    "            \n",
    "            # Text MixUp (interpolate between samples)\n",
    "            if random.random() < 0.5:\n",
    "                texts, labels = mixup_data(texts, labels, alpha=0.2)\n",
    "            \n",
    "            outputs, _ = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Add L2 regularization manually (extra strong)\n",
    "            l2_reg = 0.0\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param, 2)\n",
    "            loss = loss + 0.001 * l2_reg\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_f1 = validate(model, val_loader, criterion)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_f1)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ddd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class BalancedBERT(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized for balanced dataset of ~12K samples\n",
    "    Larger capacity than previous models but carefully regularized\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_classes=5, max_len=128,\n",
    "                 hidden_size=256, num_layers=4, num_heads=8,\n",
    "                 intermediate_size=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings (slightly larger for balanced data)\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_len, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(2, hidden_size)\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer layers with residual connections\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            BalancedTransformerLayer(hidden_size, num_heads, intermediate_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Multi-head attention pooling (better than just CLS)\n",
    "        self.attention_pool = MultiHeadAttentionPooling(hidden_size, num_heads=4)\n",
    "        \n",
    "        # Enhanced classifier with multiple residual blocks\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.8),  # Slightly less dropout in later layers\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        print(f\"BalancedBERT parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Better initialization for balanced data\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=0.7)  # Lower gain\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, \n",
    "                                   device=input_ids.device).unsqueeze(0)\n",
    "        \n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        # Embeddings\n",
    "        words_embeddings = self.embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        # Prepare attention mask\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != 0)\n",
    "        \n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask.float()) * -10000.0\n",
    "        \n",
    "        # Encoder layers with residual connections\n",
    "        hidden_states = embeddings\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(hidden_states, extended_attention_mask)\n",
    "        \n",
    "        # Attention pooling over all tokens (better representation)\n",
    "        pooled_output = self.attention_pool(hidden_states, attention_mask)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class BalancedTransformerLayer(nn.Module):\n",
    "    \"\"\"Enhanced transformer layer with pre-norm and better initialization\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, intermediate_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            hidden_size, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network with gated linear unit\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size * 2),\n",
    "            nn.GLU(dim=-1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(intermediate_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Learnable scaling factors (for better gradient flow)\n",
    "        self.gamma1 = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.gamma2 = nn.Parameter(torch.ones(hidden_size))\n",
    "    \n",
    "    def forward(self, x, attention_mask):\n",
    "        # Pre-norm self-attention\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output, _ = self.self_attn(\n",
    "            x_norm, x_norm, x_norm,\n",
    "            key_padding_mask=(attention_mask.squeeze(1).squeeze(1) == 0)\n",
    "        )\n",
    "        x = x + self.dropout1(attn_output) * self.gamma1\n",
    "        \n",
    "        # Pre-norm feed-forward\n",
    "        x_norm = self.norm2(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = x + self.dropout2(ffn_output) * self.gamma2\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttentionPooling(nn.Module):\n",
    "    \"\"\"Context-aware attention pooling\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Learnable query\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        batch_size = hidden_states.size(0)\n",
    "        \n",
    "        # Expand learnable query\n",
    "        query = self.query.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Project\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(hidden_states)\n",
    "        V = self.v_linear(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(\n",
    "                attention_mask.squeeze(1).squeeze(1).unsqueeze(1).unsqueeze(2) == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.num_heads * self.head_dim\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        pooled = self.out_proj(context).squeeze(1)\n",
    "        \n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of augmentation helpers. Set run_examples=True to execute\n",
    "# (downloads models: nlpaug BERT, MarianMT for back-translation).\n",
    "run_examples = True\n",
    "\n",
    "if run_examples:\n",
    "    sample_texts = df['text'].dropna().sample(2, random_state=0).tolist()\n",
    "\n",
    "    print(\"BERT insert augmentation:\")\n",
    "    bert_aug = helper.augment_with_bert_insert(\n",
    "        sample_texts,\n",
    "        model_path='bert-base-uncased',\n",
    "        n=1,\n",
    "        aug_p=0.2,\n",
    "        action='insert',\n",
    "    )\n",
    "    for original, augmented in zip(sample_texts, bert_aug):\n",
    "        print(\"- Original:\", original)\n",
    "        print(\"  Augmented:\", augmented)\n",
    "else:\n",
    "    print(\"Set run_examples=True to run augmentation demos (will download models).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if dataset is large enough for training embeddings from scratch\n",
    "# # Requirement: >50k samples for training embeddings from scratch\n",
    "\n",
    "# THRESHOLD = 50000\n",
    "# dataset_size = len(df)\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"DATASET SIZE ASSESSMENT FOR EMBEDDING TRAINING\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"\\nCurrent dataset size: {dataset_size:,} samples\")\n",
    "# print(f\"Required threshold: {THRESHOLD:,} samples\")\n",
    "# print(f\"\\nDifference: {dataset_size - THRESHOLD:,} samples\")\n",
    "# print(f\"Percentage of threshold: {(dataset_size / THRESHOLD) * 100:.2f}%\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# if dataset_size >= THRESHOLD:\n",
    "#     print(\"‚úÖ SUFFICIENT: Dataset meets the requirement for training embeddings from scratch\")\n",
    "# else:\n",
    "#     print(\"‚ùå INSUFFICIENT: Dataset is below the recommended threshold\")\n",
    "#     print(f\"   You need {THRESHOLD - dataset_size:,} more samples to meet the requirement\")\n",
    "#     print(\"\\n   Recommendations:\")\n",
    "#     print(\"   - Consider data augmentation techniques\")\n",
    "#     print(\"   - Look for additional data sources\")\n",
    "#     print(\"   - Use smaller embedding dimensions if training anyway\")\n",
    "#     print(\"   - Consider using pre-trained embeddings (if allowed by project constraints)\")\n",
    "# print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ae7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Dataset Size Information:\")\n",
    "# print(f\"Shape (rows, columns): {df.shape}\")\n",
    "# print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "# print(f\"Number of columns: {df.shape[1]}\")\n",
    "# print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "# print(f\"\\nMemory usage:\")\n",
    "# print(df.memory_usage(deep=True))\n",
    "# print(f\"\\nTotal memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f392f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate dictionary/vocabulary size (number of unique words)\n",
    "# # This is crucial for embedding training: vocab_size √ó embedding_dim = embedding matrix size\n",
    "\n",
    "# from collections import Counter\n",
    "# import re\n",
    "\n",
    "# # Use cleaned text if available, otherwise use original text\n",
    "# text_column = 'text_no_urls' if 'text_no_urls' in df.columns else 'text'\n",
    "\n",
    "# # Combine all text and convert to lowercase\n",
    "# all_text = ' '.join(df[text_column].astype(str))\n",
    "\n",
    "# # Tokenize: split by whitespace and remove punctuation\n",
    "# words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
    "\n",
    "# # Count unique words (dictionary/vocabulary size)\n",
    "# unique_words = set(words)\n",
    "# vocab_size = len(unique_words)\n",
    "# word_counts = Counter(words)\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"DICTIONARY/VOCABULARY SIZE ANALYSIS\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"\\nüìö Dictionary Size (Vocabulary Size): {vocab_size:,} unique words\")\n",
    "# print(f\"üìù Total word tokens: {len(words):,}\")\n",
    "# print(f\"üìä Average words per review: {len(words) / len(df):.2f}\")\n",
    "# print(f\"üìà Vocabulary coverage: {(vocab_size / len(words)) * 100:.4f}% (unique/total)\")\n",
    "\n",
    "# print(f\"\\nüîù Most frequent words (top 20):\")\n",
    "# for i, (word, count) in enumerate(word_counts.most_common(20), 1):\n",
    "#     percentage = (count / len(words)) * 100\n",
    "#     print(f\"  {i:2d}. {word:15s} : {count:8,} occurrences ({percentage:5.2f}%)\")\n",
    "\n",
    "# print(f\"\\nüí° Embedding Matrix Size Estimation:\")\n",
    "# print(f\"   For embedding_dim = 100: {vocab_size:,} √ó 100 = {vocab_size * 100:,} parameters\")\n",
    "# print(f\"   For embedding_dim = 200: {vocab_size:,} √ó 200 = {vocab_size * 200:,} parameters\")\n",
    "# print(f\"   For embedding_dim = 300: {vocab_size:,} √ó 300 = {vocab_size * 300:,} parameters\")\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4727e3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5772db1601e74c6ba65a5732c0daa18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48d9c2d89c4414193f0590409099d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde61244a73945bfacdac39b3aa5866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7784ec4649f413c8abf57b394432785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8a463c243745de80150f7fc0f7e3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, cls.predictions.transform.dense.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.output.dense.bias, cls.predictions.transform.LayerNorm.bias, cls.predictions.transform.LayerNorm.weight, bert.encoder.layer.*.attention.self.key.bias, bert.embeddings.position_embeddings.weight, bert.encoder.layer.*.attention.self.value.bias, bert.encoder.layer.*.attention.self.key.weight, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, cls.predictions.transform.dense.bias, bert.encoder.layer.*.attention.self.query.weight, cls.predictions.decoder.bias, bert.embeddings.LayerNorm.weight, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.bias, cls.predictions.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.output.LayerNorm.weight, cls.predictions.decoder.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.embeddings.word_embeddings.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "Augmented Texts:\n",
      "1: the big quick talking brown fox jumps over the lazy dog.\n",
      "2: the quick and brown white fox jumps over the lazy dog.\n",
      "3: lucky the cute quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Use BERT word-level insert (fast and avoids XLNet tokenization bug)\n",
    "augmented_texts = helper.augment_with_bert_insert(\n",
    "    [text],\n",
    "    model_path=\"bert-base-uncased\",\n",
    "    n=3,\n",
    "    aug_p=0.2,\n",
    "    action=\"insert\",\n",
    ")\n",
    "\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Texts:\")\n",
    "for i, aug in enumerate(augmented_texts, 1):\n",
    "    print(f\"{i}: {aug}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b610489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f920e8",
   "metadata": {},
   "source": [
    "# Save trained TextCNNGLU model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a563c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model checkpoint saved to textcnn_best.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model checkpoint\n",
    "# Includes state_dict, vocab, and label_map for later reuse\n",
    "\n",
    "model_ckpt_path = \"textcnn_best.pt\"\n",
    "\n",
    "if 'model' in locals() and 'vocab' in locals() and 'label_map' in locals():\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"vocab\": vocab,\n",
    "        \"label_map\": label_map,\n",
    "    }, model_ckpt_path)\n",
    "    print(f\"‚úÖ Model checkpoint saved to {model_ckpt_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model, vocab, or label_map not found. Train the model first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4976c73",
   "metadata": {},
   "source": [
    "# Generate submission on test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb9f96b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ submission.csv saved (3000 rows)\n",
      "     id     review\n",
      "0   298  Excellent\n",
      "1  4153       Good\n",
      "2  5359       Good\n",
      "3  7734        Bad\n",
      "4  3283  Very good\n"
     ]
    }
   ],
   "source": [
    "# Create submission.csv using the trained TextCNNGLU model\n",
    "# Expects test.csv with a 'text' column (and optional 'id')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "submission_path = \"submission.csv\"\n",
    "test_path = \"test.csv\"\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    print(f\"‚ö†Ô∏è {test_path} not found. Place your test file in the project root.\")\n",
    "elif 'model' not in locals() or 'vocab' not in locals() or 'label_map' not in locals():\n",
    "    print(\"‚ö†Ô∏è Model, vocab, or label_map not found. Train (or load) the model first.\")\n",
    "else:\n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    if 'text' not in test_df.columns:\n",
    "        raise ValueError(\"test.csv must contain a 'text' column\")\n",
    "\n",
    "    # Encode test texts\n",
    "    encoded_test = [main.preprocess_text_for_small_data(t, vocab, max_len=150) for t in test_df['text'].fillna('')]\n",
    "    X_test_submit = torch.tensor(encoded_test, dtype=torch.long)\n",
    "    test_loader_submit = DataLoader(TensorDataset(X_test_submit), batch_size=64, shuffle=False)\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    inv_label_map = {v: k for k, v in label_map.items()}\n",
    "    with torch.no_grad():\n",
    "        for (xb,) in test_loader_submit:\n",
    "            xb = xb.to(device)\n",
    "            logits, _ = model(xb)\n",
    "            pred_ids = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend([inv_label_map[p] for p in pred_ids])\n",
    "\n",
    "    # Build submission\n",
    "    if 'id' in test_df.columns:\n",
    "        submission = pd.DataFrame({'id': test_df['id'], 'review': preds})\n",
    "    else:\n",
    "        submission = pd.DataFrame({'review': preds})\n",
    "\n",
    "    submission.to_csv(submission_path, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ submission.csv saved ({len(submission)} rows)\")\n",
    "    print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4e4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
