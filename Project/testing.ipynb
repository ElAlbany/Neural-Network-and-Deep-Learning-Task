{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ddd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class BalancedBERT(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized for balanced dataset of ~12K samples\n",
    "    Larger capacity than previous models but carefully regularized\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_classes=5, max_len=128,\n",
    "                 hidden_size=256, num_layers=4, num_heads=8,\n",
    "                 intermediate_size=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings (slightly larger for balanced data)\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_len, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(2, hidden_size)\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer layers with residual connections\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            BalancedTransformerLayer(hidden_size, num_heads, intermediate_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Multi-head attention pooling (better than just CLS)\n",
    "        self.attention_pool = MultiHeadAttentionPooling(hidden_size, num_heads=4)\n",
    "        \n",
    "        # Enhanced classifier with multiple residual blocks\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.8),  # Slightly less dropout in later layers\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        print(f\"BalancedBERT parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Better initialization for balanced data\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=0.7)  # Lower gain\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, \n",
    "                                   device=input_ids.device).unsqueeze(0)\n",
    "        \n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        # Embeddings\n",
    "        words_embeddings = self.embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        # Prepare attention mask\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != 0)\n",
    "        \n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask.float()) * -10000.0\n",
    "        \n",
    "        # Encoder layers with residual connections\n",
    "        hidden_states = embeddings\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(hidden_states, extended_attention_mask)\n",
    "        \n",
    "        # Attention pooling over all tokens (better representation)\n",
    "        pooled_output = self.attention_pool(hidden_states, attention_mask)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class BalancedTransformerLayer(nn.Module):\n",
    "    \"\"\"Enhanced transformer layer with pre-norm and better initialization\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, intermediate_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            hidden_size, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network with gated linear unit\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size * 2),\n",
    "            nn.GLU(dim=-1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(intermediate_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Learnable scaling factors (for better gradient flow)\n",
    "        self.gamma1 = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.gamma2 = nn.Parameter(torch.ones(hidden_size))\n",
    "    \n",
    "    def forward(self, x, attention_mask):\n",
    "        # Pre-norm self-attention\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output, _ = self.self_attn(\n",
    "            x_norm, x_norm, x_norm,\n",
    "            key_padding_mask=(attention_mask.squeeze(1).squeeze(1) == 0)\n",
    "        )\n",
    "        x = x + self.dropout1(attn_output) * self.gamma1\n",
    "        \n",
    "        # Pre-norm feed-forward\n",
    "        x_norm = self.norm2(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = x + self.dropout2(ffn_output) * self.gamma2\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttentionPooling(nn.Module):\n",
    "    \"\"\"Context-aware attention pooling\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Learnable query\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        batch_size = hidden_states.size(0)\n",
    "        \n",
    "        # Expand learnable query\n",
    "        query = self.query.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Project\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(hidden_states)\n",
    "        V = self.v_linear(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(\n",
    "                attention_mask.squeeze(1).squeeze(1).unsqueeze(1).unsqueeze(2) == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.num_heads * self.head_dim\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        pooled = self.out_proj(context).squeeze(1)\n",
    "        \n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of augmentation helpers. Set run_examples=True to execute\n",
    "# (downloads models: nlpaug BERT, MarianMT for back-translation).\n",
    "run_examples = True\n",
    "\n",
    "if run_examples:\n",
    "    sample_texts = df['text'].dropna().sample(2, random_state=0).tolist()\n",
    "\n",
    "    print(\"BERT insert augmentation:\")\n",
    "    bert_aug = helper.augment_with_bert_insert(\n",
    "        sample_texts,\n",
    "        model_path='bert-base-uncased',\n",
    "        n=1,\n",
    "        aug_p=0.2,\n",
    "        action='insert',\n",
    "    )\n",
    "    for original, augmented in zip(sample_texts, bert_aug):\n",
    "        print(\"- Original:\", original)\n",
    "        print(\"  Augmented:\", augmented)\n",
    "else:\n",
    "    print(\"Set run_examples=True to run augmentation demos (will download models).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if dataset is large enough for training embeddings from scratch\n",
    "# # Requirement: >50k samples for training embeddings from scratch\n",
    "\n",
    "# THRESHOLD = 50000\n",
    "# dataset_size = len(df)\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"DATASET SIZE ASSESSMENT FOR EMBEDDING TRAINING\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"\\nCurrent dataset size: {dataset_size:,} samples\")\n",
    "# print(f\"Required threshold: {THRESHOLD:,} samples\")\n",
    "# print(f\"\\nDifference: {dataset_size - THRESHOLD:,} samples\")\n",
    "# print(f\"Percentage of threshold: {(dataset_size / THRESHOLD) * 100:.2f}%\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# if dataset_size >= THRESHOLD:\n",
    "#     print(\"‚úÖ SUFFICIENT: Dataset meets the requirement for training embeddings from scratch\")\n",
    "# else:\n",
    "#     print(\"‚ùå INSUFFICIENT: Dataset is below the recommended threshold\")\n",
    "#     print(f\"   You need {THRESHOLD - dataset_size:,} more samples to meet the requirement\")\n",
    "#     print(\"\\n   Recommendations:\")\n",
    "#     print(\"   - Consider data augmentation techniques\")\n",
    "#     print(\"   - Look for additional data sources\")\n",
    "#     print(\"   - Use smaller embedding dimensions if training anyway\")\n",
    "#     print(\"   - Consider using pre-trained embeddings (if allowed by project constraints)\")\n",
    "# print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ae7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Dataset Size Information:\")\n",
    "# print(f\"Shape (rows, columns): {df.shape}\")\n",
    "# print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "# print(f\"Number of columns: {df.shape[1]}\")\n",
    "# print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "# print(f\"\\nMemory usage:\")\n",
    "# print(df.memory_usage(deep=True))\n",
    "# print(f\"\\nTotal memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f392f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate dictionary/vocabulary size (number of unique words)\n",
    "# # This is crucial for embedding training: vocab_size √ó embedding_dim = embedding matrix size\n",
    "\n",
    "# from collections import Counter\n",
    "# import re\n",
    "\n",
    "# # Use cleaned text if available, otherwise use original text\n",
    "# text_column = 'text_no_urls' if 'text_no_urls' in df.columns else 'text'\n",
    "\n",
    "# # Combine all text and convert to lowercase\n",
    "# all_text = ' '.join(df[text_column].astype(str))\n",
    "\n",
    "# # Tokenize: split by whitespace and remove punctuation\n",
    "# words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
    "\n",
    "# # Count unique words (dictionary/vocabulary size)\n",
    "# unique_words = set(words)\n",
    "# vocab_size = len(unique_words)\n",
    "# word_counts = Counter(words)\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"DICTIONARY/VOCABULARY SIZE ANALYSIS\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"\\nüìö Dictionary Size (Vocabulary Size): {vocab_size:,} unique words\")\n",
    "# print(f\"üìù Total word tokens: {len(words):,}\")\n",
    "# print(f\"üìä Average words per review: {len(words) / len(df):.2f}\")\n",
    "# print(f\"üìà Vocabulary coverage: {(vocab_size / len(words)) * 100:.4f}% (unique/total)\")\n",
    "\n",
    "# print(f\"\\nüîù Most frequent words (top 20):\")\n",
    "# for i, (word, count) in enumerate(word_counts.most_common(20), 1):\n",
    "#     percentage = (count / len(words)) * 100\n",
    "#     print(f\"  {i:2d}. {word:15s} : {count:8,} occurrences ({percentage:5.2f}%)\")\n",
    "\n",
    "# print(f\"\\nüí° Embedding Matrix Size Estimation:\")\n",
    "# print(f\"   For embedding_dim = 100: {vocab_size:,} √ó 100 = {vocab_size * 100:,} parameters\")\n",
    "# print(f\"   For embedding_dim = 200: {vocab_size:,} √ó 200 = {vocab_size * 200:,} parameters\")\n",
    "# print(f\"   For embedding_dim = 300: {vocab_size:,} √ó 300 = {vocab_size * 300:,} parameters\")\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4727e3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5772db1601e74c6ba65a5732c0daa18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48d9c2d89c4414193f0590409099d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde61244a73945bfacdac39b3aa5866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7784ec4649f413c8abf57b394432785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8a463c243745de80150f7fc0f7e3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, cls.predictions.transform.dense.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.output.dense.bias, cls.predictions.transform.LayerNorm.bias, cls.predictions.transform.LayerNorm.weight, bert.encoder.layer.*.attention.self.key.bias, bert.embeddings.position_embeddings.weight, bert.encoder.layer.*.attention.self.value.bias, bert.encoder.layer.*.attention.self.key.weight, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, cls.predictions.transform.dense.bias, bert.encoder.layer.*.attention.self.query.weight, cls.predictions.decoder.bias, bert.embeddings.LayerNorm.weight, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.bias, cls.predictions.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.output.LayerNorm.weight, cls.predictions.decoder.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.embeddings.word_embeddings.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "Augmented Texts:\n",
      "1: the big quick talking brown fox jumps over the lazy dog.\n",
      "2: the quick and brown white fox jumps over the lazy dog.\n",
      "3: lucky the cute quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Use BERT word-level insert (fast and avoids XLNet tokenization bug)\n",
    "augmented_texts = helper.augment_with_bert_insert(\n",
    "    [text],\n",
    "    model_path=\"bert-base-uncased\",\n",
    "    n=3,\n",
    "    aug_p=0.2,\n",
    "    action=\"insert\",\n",
    ")\n",
    "\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Texts:\")\n",
    "for i, aug in enumerate(augmented_texts, 1):\n",
    "    print(f\"{i}: {aug}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b610489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
