{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7aa64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import importlib\n",
    "importlib.reload(helper)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "import sentencepiece as spm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1e75645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"train.csv\")\n",
    "df = df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b187268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Excellent', 'Very good', 'Bad', 'Good', 'Very bad'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9bf42",
   "metadata": {},
   "source": [
    "# Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfccfa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews with URLs: 140\n",
      "Percentage: 2.00%\n"
     ]
    }
   ],
   "source": [
    "df_with_url_detection = helper.apply_url_processing(df, text_column='text', operation='detect')\n",
    "print(f\"Number of reviews with URLs: {df_with_url_detection['has_url'].sum()}\")\n",
    "print(f\"Percentage: {df_with_url_detection['has_url'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30900b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Created 'text_no_urls' column with URLs removed\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_cleaned = helper.apply_url_processing(df, text_column='text', operation='remove', replacement='')\n",
    "print(f\"\\n✅ Created 'text_no_urls' column with URLs removed\")\n",
    "\n",
    "# # Show examples if URLs were found\n",
    "# if df_with_url_detection['has_url'].any():\n",
    "#     print(f\"\\nExamples of cleaned text (first 3 with URLs):\")\n",
    "#     sample_indices = df_with_url_detection[df_with_url_detection['has_url']].head(3).index\n",
    "#     for idx in sample_indices:\n",
    "#         original = df.loc[idx, 'text'][:150]\n",
    "#         cleaned = df_cleaned.loc[idx, 'text'][:150]\n",
    "#         print(f\"\\nOriginal: {original}...\")\n",
    "#         print(f\"Cleaned:  {cleaned}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e6f0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28ce6c",
   "metadata": {},
   "source": [
    "# Converting to LowerCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27957d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab50f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    honestly the best part of this place is the unbelievable deal you can get on pet related products. because they are one of the primary income sources for halo animal rescue, people that have pets, tend to make donations. you can almost always find pet carriers for under $10. they have dog crates for training and you don't have to shell out a fortune for them. they have fish tanks that will allow you to let little johnny get his feet wet in the world of pet care and the associated responsibilities without breaking the bank. if you are a person that shares your life with animals, you understand the rarity of these kinds of deals!\\n\\nyes they have clothes, furniture, tchatchkies, etc., but the real deals are on the pet products.\\n\\nas a side note, halo recently leased the mccac location at 5231 north 35th avenue (it was the previous maricopa county run cat adoption center). so if you are looking for a companion (cat or dog), check them out and support one of the fastest growing no-kill efforts in the us.\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = df.loc[df['id'] == 7961,'text']\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5d3c3",
   "metadata": {},
   "source": [
    "# Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2d66070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution (count, percent):\n",
      "           count    percent\n",
      "review                     \n",
      "Bad          648   9.257143\n",
      "Excellent   2335  33.357143\n",
      "Good        1024  14.628571\n",
      "Very bad     524   7.485714\n",
      "Very good   2469  35.271429\n",
      "\n",
      "Total samples: 7,000\n",
      "Imbalance ratio (max/min): 4.71\n"
     ]
    }
   ],
   "source": [
    "# Calculate counts and percentages of target classes\n",
    "summary = helper.target_distribution(df, target_col=\"review\")\n",
    "\n",
    "print(\"Target distribution (count, percent):\")\n",
    "print(summary)\n",
    "print(f\"\\nTotal samples: {len(df):,}\")\n",
    "print(\n",
    "    f\"Imbalance ratio (max/min): {summary['count'].max() / summary['count'].min():.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40137333",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605616fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts:\n",
      "            count    percent\n",
      "review                     \n",
      "Bad          648   9.257143\n",
      "Excellent   2335  33.357143\n",
      "Good        1024  14.628571\n",
      "Very bad     524   7.485714\n",
      "Very good   2469  35.271429\n",
      "\n",
      "Augmented counts:\n",
      "            count    percent\n",
      "review                     \n",
      "Bad         2469  20.219474\n",
      "Excellent   2335  19.122103\n",
      "Good        2469  20.219474\n",
      "Very bad    2469  20.219474\n",
      "Very good   2469  20.219474\n",
      "\n",
      "Total rows before: 7,000 | after: 12,211\n"
     ]
    }
   ],
   "source": [
    "# # Set to True to augment minority classes (Bad, Good, Very bad)\n",
    "# run_augment = True\n",
    "# if run_augment:\n",
    "#     classes_to_augment = [\"Bad\", \"Good\", \"Very bad\"]\n",
    "#     df_augmented = helper.augment_classes(\n",
    "#         df,\n",
    "#         text_col=\"text\",\n",
    "#         target_col=\"review\",\n",
    "#         classes=classes_to_augment,\n",
    "#         augment_fn=lambda texts: helper.augment_with_bert_insert(\n",
    "#             texts,\n",
    "#             model_path=\"bert-base-uncased\",\n",
    "#             n=1,\n",
    "#             aug_p=0.2,\n",
    "#             action=\"insert\",\n",
    "#         ),\n",
    "#         n_per_sample=1,\n",
    "#         target_count=None,  # defaults to current max class size  \n",
    "#         random_state=42,\n",
    "#     )\n",
    "\n",
    "#     print(\"Original counts:\\n\", helper.target_distribution(df, \"review\"))\n",
    "#     print(\"\\nAugmented counts:\\n\", helper.target_distribution(df_augmented, \"review\"))\n",
    "#     print(f\"\\nTotal rows before: {len(df):,} | after: {len(df_augmented):,}\")\n",
    "\n",
    "#     # If you want to continue with augmented data:\n",
    "#     df = df_augmented\n",
    "# else:\n",
    "#     print(\"Set run_augment=True to perform class-specific augmentation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd6ad0",
   "metadata": {},
   "source": [
    "# Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5190f880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Augmented data saved to: train_augmented.csv\n",
      "   Total rows: 12,211\n",
      "   Columns: ['id', 'text', 'review']\n",
      "   File size: 9.25 MB\n",
      "\n",
      "   Preview (first 3 rows):\n",
      "       id  \\\n",
      "0  7961.0   \n",
      "1  4697.0   \n",
      "2  4459.0   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
      "0                                                                                           honestly the best part of this place is the unbelievable deal you can get on pet related products. because they are one of the primary income sources for halo animal rescue, people that have pets, tend to make donations. you can almost always find pet carriers for under $10. they have dog crates for training and you don't have to shell out a fortune for them. they have fish tanks that will allow you to let little johnny get his feet wet in the world of pet care and the associated responsibilities without breaking the bank. if you are a person that shares your life with animals, you understand the rarity of these kinds of deals!\\n\\nyes they have clothes, furniture, tchatchkies, etc., but the real deals are on the pet products.\\n\\nas a side note, halo recently leased the mccac location at 5231 north 35th avenue (it was the previous maricopa county run cat adoption center). so if you are looking for a companion (cat or dog), check them out and support one of the fastest growing no-kill efforts in the us.   \n",
      "1                                                                                                                                                                               found indulge on a whim, based on their huge \"gluten-free menu\" outside - i was not disappointed when we decided to try it!\\n\\nindulge has a separate gluten-free menu and the server was very knowledgable walking me through their kitchen processes to reassure me.  \\n\\ni ordered my burger on a gluten-free bun (!!!) the first time i was ever able to do this in a restaurant.  that was exciting!\\n\\ni think indulge is as good as you make it - you are given a menu to check off ingredients for your burger.  they have sooo many choices, that it's likely that if you don't like your burger, you made the wrong choice. \\n\\ni had no such problem - i loved my custom burger!  it was cooked to the correct temperature, was delicious & best of all, was delivered on a tasty gluten-free bun. \\n\\nplus, though i was full (it was a lot of food!) i was able to take a gluten-free cupcake to go - wow, was that ever heaven later!  thanks indulge!   \n",
      "2  my take on mill street is that it's your classic college-town main strip area with a couple bars with big tv's, a burrito place, a burger place, the quintessential late night grease-craving joint and a couple \"edgy\" \"boutiques\" selling overpriced asian imports to tourists that will bite (guilty), and college students trying to reinvent themselves with \"hippie-chic,\" whatever the f that is. (um, also guilty...)\\n\\nbut i way digress! streets like this also always have a couple more decorous dining establishments for when the parental units are in town -- which is exactly what i happen to have stumbled upon in this place, and perfect because if there's anything i love as much as edgy \"hippie chic,\" it's decorum.\\n\\ni treated myself to my one sit-down meal during a week of travel/ being late for everything/ conference bagged lunches, and can't think of much more to have asked for than sitting outside on their little street-side patio with my glass of rosé and my truffle oil pizza. granted i've had better rosé and more perceptive service, but man, truffle oil. self indulgent afternoon success.   \n",
      "\n",
      "      review  \n",
      "0  Excellent  \n",
      "1  Excellent  \n",
      "2  Very good  \n"
     ]
    }
   ],
   "source": [
    "# Save augmented dataset to CSV file\n",
    "save_augmented = True\n",
    "output_filename = 'train_augmented.csv'  # Change filename if needed\n",
    "\n",
    "if save_augmented:\n",
    "    # Use df_augmented if it exists, otherwise use df (if augmentation was applied)\n",
    "    if 'df_augmented' in locals():\n",
    "        data_to_save = df_augmented\n",
    "    elif run_augment and len(df) > len(df_original):\n",
    "        # df was updated with augmented data\n",
    "        data_to_save = df\n",
    "    else:\n",
    "        data_to_save = None\n",
    "        print(\"⚠️ No augmented data found. Run augmentation first (set run_augment=True).\")\n",
    "    \n",
    "    if data_to_save is not None:\n",
    "        # Save to CSV\n",
    "        data_to_save.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"✅ Augmented data saved to: {output_filename}\")\n",
    "        print(f\"   Total rows: {len(data_to_save):,}\")\n",
    "        print(f\"   Columns: {list(data_to_save.columns)}\")\n",
    "        \n",
    "        if os.path.exists(output_filename):\n",
    "            file_size_mb = os.path.getsize(output_filename) / 1024 / 1024\n",
    "            print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Show a preview\n",
    "        print(f\"\\n   Preview (first 3 rows):\")\n",
    "        print(data_to_save.head(3))\n",
    "else:\n",
    "    print(\"Set save_augmented=True to save the augmented data to CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c47b4c",
   "metadata": {},
   "source": [
    "# Custom Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bab68f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on 12,211 texts...\n",
      "Method: bpe, Vocab size: 8000\n",
      "\n",
      "============================================================\n",
      "Sample encodings:\n",
      "============================================================\n",
      "\n",
      "1. Original: honestly the best part of this place is the unbelievable deal you can get on pet...\n",
      "   Encoded (first 20 tokens): [1746, 111, 475, 660, 135, 179, 218, 126, 111, 4403, 959, 156, 263, 229, 118, 1709, 6275, 2553, 18, 362]\n",
      "   Length: 128\n",
      "\n",
      "2. Original: found indulge on a whim, based on their huge \"gluten-free menu\" outside - i was ...\n",
      "   Encoded (first 20 tokens): [869, 6680, 118, 44, 7769, 16, 1754, 118, 283, 902, 6, 3359, 17, 827, 465, 6, 907, 17, 52, 139]\n",
      "   Length: 128\n",
      "\n",
      "3. Original: my take on mill street is that it's your classic college-town main strip area wi...\n",
      "   Encoded (first 20 tokens): [164, 520, 118, 1624, 1270, 126, 161, 124, 11, 62, 313, 2881, 2281, 17, 716, 1088, 2266, 644, 175, 44]\n",
      "   Length: 128\n",
      "\n",
      "✅ Tokenizer ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Train custom tokenizer on your corpus\n",
    "# Options: 'bpe', 'sentencepiece', 'wordpiece'\n",
    "# SentencePiece is recommended for smaller datasets\n",
    "\n",
    "train_tokenizer = True\n",
    "tokenizer_method = 'bpe'  # or 'bpe', 'wordpiece'\n",
    "vocab_size = 8000\n",
    "output_dir = './tokenizers'\n",
    "\n",
    "if train_tokenizer:\n",
    "    # Get all texts from dataframe\n",
    "    texts = df['text'].dropna().tolist()\n",
    "    print(f\"Training tokenizer on {len(texts):,} texts...\")\n",
    "    print(f\"Method: {tokenizer_method}, Vocab size: {vocab_size}\")\n",
    "    \n",
    "    # Create and train tokenizer\n",
    "    tokenizer = helper.CustomTokenizer(\n",
    "        vocab_size=vocab_size,\n",
    "        method=tokenizer_method\n",
    "    )\n",
    "    tokenizer.train_from_texts(texts, output_dir=output_dir)\n",
    "    \n",
    "    # Test encoding\n",
    "    sample_texts = df['text'].head(3).tolist()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample encodings:\")\n",
    "    print(\"=\"*60)\n",
    "    for i, text in enumerate(sample_texts, 1):\n",
    "        encoded = tokenizer.encode(text, max_length=128)\n",
    "        print(f\"\\n{i}. Original: {text[:80]}...\")\n",
    "        print(f\"   Encoded (first 20 tokens): {encoded[:20]}\")\n",
    "        print(f\"   Length: {len([t for t in encoded if t != 0])}\")\n",
    "    \n",
    "    print(\"\\n✅ Tokenizer ready to use!\")\n",
    "else:\n",
    "    print(\"Set train_tokenizer=True to train the custom tokenizer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9970b",
   "metadata": {},
   "source": [
    "# Model Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1b8ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading augmented data from train_augmented.csv...\n",
      "\n",
      "Encoding 12,211 texts...\n",
      "✅ Data prepared:\n",
      "   Texts encoded: torch.Size([12211, 128])\n",
      "   Labels shape: torch.Size([12211])\n",
      "   Label distribution:\n",
      "     Very bad: 2,469 (20.2%)\n",
      "     Bad: 2,469 (20.2%)\n",
      "     Good: 2,469 (20.2%)\n",
      "     Very good: 2,469 (20.2%)\n",
      "     Excellent: 2,335 (19.1%)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "# Load augmented data if available, otherwise use current df\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load augmented data or use current dataframe\n",
    "if os.path.exists('train_augmented.csv'):\n",
    "    print(\"Loading augmented data from train_augmented.csv...\")\n",
    "    df_train = pd.read_csv('train_augmented.csv')\n",
    "else:\n",
    "    print(\"Using current dataframe (no augmented file found)...\")\n",
    "    df_train = df.copy()\n",
    "\n",
    "# Encode texts using trained tokenizer\n",
    "if 'tokenizer' not in locals():\n",
    "    print(\"⚠️ Tokenizer not found! Train tokenizer first.\")\n",
    "    print(\"   Loading saved tokenizer...\")\n",
    "    # Try to load saved tokenizer\n",
    "    if os.path.exists('./tokenizers/bpe_tokenizer.json'):\n",
    "        from tokenizers import Tokenizer\n",
    "        loaded_tokenizer = Tokenizer.from_file('./tokenizers/bpe_tokenizer.json')\n",
    "        # Wrap in CustomTokenizer for consistent interface\n",
    "        tokenizer = helper.CustomTokenizer(vocab_size=8000, method='bpe')\n",
    "        tokenizer.tokenizer = loaded_tokenizer\n",
    "        print(\"✅ Loaded saved tokenizer\")\n",
    "    else:\n",
    "        raise ValueError(\"No tokenizer found. Please train tokenizer first.\")\n",
    "\n",
    "# Encode all texts\n",
    "print(f\"\\nEncoding {len(df_train):,} texts...\")\n",
    "texts = df_train['text'].dropna().tolist()\n",
    "\n",
    "# Use CustomTokenizer's encode_batch if available, otherwise handle HuggingFace tokenizer\n",
    "if isinstance(tokenizer, helper.CustomTokenizer):\n",
    "    encoded_texts = tokenizer.encode_batch(texts, max_length=128, return_tensors='pt')\n",
    "else:\n",
    "    # Handle HuggingFace tokenizer directly\n",
    "    encodings = tokenizer.encode_batch(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    encoded_texts = torch.tensor([enc.ids for enc in encodings])\n",
    "\n",
    "# Create labels mapping\n",
    "label_map = {'Very bad': 0, 'Bad': 1, 'Good': 2, 'Very good': 3, 'Excellent': 4}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "labels = torch.tensor([label_map[label] for label in df_train['review'].values], dtype=torch.long)\n",
    "\n",
    "print(f\"✅ Data prepared:\")\n",
    "print(f\"   Texts encoded: {encoded_texts.shape}\")\n",
    "print(f\"   Labels shape: {labels.shape}\")\n",
    "print(f\"   Label distribution:\")\n",
    "for label_name, label_id in label_map.items():\n",
    "    count = (labels == label_id).sum().item()\n",
    "    print(f\"     {label_name}: {count:,} ({count/len(labels)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe69d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data splits created:\n",
      "   Train: 8,547 samples (70.0%)\n",
      "   Validation: 1,832 samples (15.0%)\n",
      "   Test: 1,832 samples (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation/test splits\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "# First split: train vs (val + test)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    encoded_texts, labels, test_size=test_size, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Second split: train vs val\n",
    "val_size_adjusted = val_size / (train_size + val_size)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"✅ Data splits created:\")\n",
    "print(f\"   Train: {len(X_train):,} samples ({len(X_train)/len(encoded_texts)*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(X_val):,} samples ({len(X_val)/len(encoded_texts)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(X_test):,} samples ({len(X_test)/len(encoded_texts)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81279ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders created:\n",
      "   Batch size: 32\n",
      "   Train batches: 268\n",
      "   Val batches: 58\n",
      "   Test batches: 58\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Dataset and DataLoaders\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.texts[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ReviewDataset(X_train, y_train)\n",
    "val_dataset = ReviewDataset(X_val, y_val)\n",
    "test_dataset = ReviewDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"✅ DataLoaders created:\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd1a43",
   "metadata": {},
   "source": [
    "# Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8aacd50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using vocab size: 8000\n",
      "BalancedBERT parameters: 5,081,349\n",
      "\n",
      "✅ Model initialized on cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize BalancedBERT model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get vocab size from tokenizer\n",
    "if isinstance(tokenizer, helper.CustomTokenizer) and tokenizer.tokenizer is not None:\n",
    "    # For CustomTokenizer with HuggingFace tokenizer\n",
    "    if hasattr(tokenizer.tokenizer, 'get_vocab_size'):\n",
    "        vocab_size = tokenizer.tokenizer.get_vocab_size()\n",
    "    elif hasattr(tokenizer.tokenizer, 'get_vocab'):\n",
    "        vocab_size = len(tokenizer.tokenizer.get_vocab())\n",
    "    else:\n",
    "        vocab_size = tokenizer.vocab_size  # Use the vocab_size from CustomTokenizer\n",
    "elif hasattr(tokenizer, 'get_vocab_size'):\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "elif hasattr(tokenizer, 'vocab_size'):\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "else:\n",
    "    # Default fallback\n",
    "    vocab_size = 8000\n",
    "    print(f\"⚠️ Could not determine vocab size, using default: {vocab_size}\")\n",
    "\n",
    "print(f\"Using vocab size: {vocab_size}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "num_classes = 5\n",
    "max_len = 128\n",
    "hidden_size = 256\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "intermediate_size = 512\n",
    "dropout = 0.2\n",
    "\n",
    "# Create model\n",
    "model = helper.BalancedBERT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    max_len=max_len,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    intermediate_size=intermediate_size,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n✅ Model initialized on {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353507b9",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c26deb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epochs...\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "  Train Loss: nan, Train Acc: 20.21%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "  ✅ New best model saved! (Val Acc: 20.20%)\n",
      "------------------------------------------------------------\n",
      "Epoch 2/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "Epoch 3/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "Epoch 4/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "Epoch 5/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "Epoch 6/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "Epoch 7/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "Epoch 8/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "Epoch 9/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "Epoch 10/10\n",
      "  Train Loss: nan, Train Acc: 20.22%\n",
      "  Val Loss: nan, Val Acc: 20.20%\n",
      "------------------------------------------------------------\n",
      "\n",
      "✅ Training completed!\n",
      "   Best validation accuracy: 20.20%\n",
      "   Model saved to: best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "train_model = True\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "if train_model:\n",
    "    # Loss and optimizer\n",
    "    import torch.nn as nn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f\"  ✅ New best model saved! (Val Acc: {best_val_acc:.2f}%)\")\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "    \n",
    "    print(f\"\\n✅ Training completed!\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"   Model saved to: best_model.pt\")\n",
    "else:\n",
    "    print(\"Set train_model=True to start training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35afb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "if train_model and os.path.exists('best_model.pt'):\n",
    "    import torch.nn as nn\n",
    "    print(\"Loading best model for evaluation...\")\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_accuracy = accuracy_score(all_labels, all_predictions) * 100\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TEST SET RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        all_labels, all_predictions,\n",
    "        target_names=['Very bad', 'Bad', 'Good', 'Very good', 'Excellent']\n",
    "    ))\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    print(cm)\n",
    "else:\n",
    "    print(\"Train model first (set train_model=True in previous cell).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1017a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Encode batches for model training\n",
    "# This is useful when preparing data for neural network training\n",
    "\n",
    "if train_tokenizer and 'tokenizer' in locals():\n",
    "    # Example batch encoding\n",
    "    batch_texts = df['text'].head(10).tolist()\n",
    "    encoded_batch = tokenizer.encode_batch(batch_texts, max_length=128, return_tensors='pt')\n",
    "    \n",
    "    print(f\"Batch shape: {encoded_batch.shape}\")\n",
    "    print(f\"Batch size: {len(batch_texts)}, Max length: 128\")\n",
    "    print(f\"\\nFirst 3 sequences (first 10 tokens each):\")\n",
    "    for i in range(min(3, len(batch_texts))):\n",
    "        print(f\"  {i+1}. {encoded_batch[i][:10].tolist()}\")\n",
    "    \n",
    "    # You can use this for training:\n",
    "    # X = tokenizer.encode_batch(df['text'].tolist(), max_length=128, return_tensors='pt')\n",
    "    # y = df['review'].map({'Very bad': 0, 'Bad': 1, 'Good': 2, 'Very good': 3, 'Excellent': 4}).values\n",
    "else:\n",
    "    print(\"Train tokenizer first (set train_tokenizer=True in previous cell).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d618a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
