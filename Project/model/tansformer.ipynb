{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931683ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 1. PREPROCESSING\n",
    "# ------------------------\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    TextVectorization, Embedding, Input,\n",
    "    LayerNormalization, MultiHeadAttention,\n",
    "    Dense, Dropout, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('train_augmented_expanded.csv')\n",
    "\n",
    "# --- CLEAN TEXT FUNCTION ---\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ' '.join(word for word in text.split() if not word.isdigit())\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Define a mapping for sentiment labels to integers\n",
    "sentiment_to_int = {'Very bad': 0, 'Bad': 1, 'Good': 2, 'Very good': 3, 'Excellent': 4}\n",
    "\n",
    "# Apply the mapping to the 'review' column\n",
    "df['review'] = df['review'].map(sentiment_to_int)\n",
    "\n",
    "# Extract training data\n",
    "train_texts = df['text'].astype(str).tolist()\n",
    "train_labels = df['review'].astype(int).tolist()\n",
    "\n",
    "# ------------------------\n",
    "# 2. TEXT VECTORIZER\n",
    "# ------------------------\n",
    "max_tokens = 20000 # vocabulary size\n",
    "max_len = 100 # FIX: shorter length for small data\n",
    "vectorizer = TextVectorization( max_tokens=max_tokens\n",
    "                              , output_sequence_length=max_len\n",
    "                                )\n",
    "vectorizer.adapt(train_texts)\n",
    "\n",
    "# ------------------------\n",
    "# 3. TRANSFORMER BLOCK\n",
    "# ------------------------\n",
    "def transformer_block(x, num_heads=2, embed_dim=64, ff_dim=128, rate=0.3):\n",
    "    # Multi-head attention\n",
    "    attn_output = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim,\n",
    "        dropout=rate    # FIX 1: dropout inside attention\n",
    "    )(x, x)\n",
    "\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "    # Feed-forward network\n",
    "    ffn = Dense(ff_dim, activation=\"relu\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-3))(out1)  # FIX 2: L2\n",
    "    ffn = Dropout(rate)(ffn)  # FIX 3: more dropout\n",
    "    ffn = Dense(embed_dim)(ffn)\n",
    "\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn)\n",
    "\n",
    "# ------------------------\n",
    "# 4. MODEL ARCHITECTURE\n",
    "# ------------------------\n",
    "num_classes = 5\n",
    "embed_dim = 32   # smaller transformer\n",
    "num_heads = 2\n",
    "ff_dim = 64\n",
    "drop_rate = 0.4\n",
    "\n",
    "inputs = Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorizer(inputs)\n",
    "x = Embedding(max_tokens, embed_dim)(x)\n",
    "\n",
    "# Transformer Block (with fixes)\n",
    "x = transformer_block(x, num_heads, embed_dim, ff_dim, drop_rate)\n",
    "\n",
    "# Pooling\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.5)(x)   # FIX 4: bigger dropout before dense\n",
    "\n",
    "# Output\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ------------------------\n",
    "# 5. TRAINING WITH VALIDATION\n",
    "# ------------------------\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data and labels into training and validation sets manually\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "# Convert text data and labels to numpy arrays, explicitly using dtype=object for strings\n",
    "X_train_split = np.array(X_train_split, dtype=object) # Convert list of strings to numpy array of strings\n",
    "X_val_split = np.array(X_val_split, dtype=object)   # Convert list of strings to numpy array of strings\n",
    "y_train_split = np.array(y_train_split)\n",
    "y_val_split = np.array(y_val_split)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_split,\n",
    "    y_train_split,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    epochs=20,             # we increase this because early stopping will stop early\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stop]  # <-- IMPORTANT\n",
    ")\n",
    "\n",
    "# ------------------------\n",
    "# 6. TEST PREDICTION\n",
    "# ------------------------\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Clean text\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Convert to numpy array of strings\n",
    "test_texts = test_df[\"text\"].astype(str).tolist()\n",
    "X_test = np.array(test_texts, dtype=object)\n",
    "\n",
    "# Predict probabilities\n",
    "pred_probs = model.predict(X_test, batch_size=32)\n",
    "\n",
    "# Convert to class labels\n",
    "pred_labels = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Reverse mapping\n",
    "int_to_sentiment = {v: k for k, v in sentiment_to_int.items()}\n",
    "pred_sentiments = [int_to_sentiment[i] for i in pred_labels]\n",
    "\n",
    "ids = test_df[\"id\"]\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"review\": pred_sentiments\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file saved as submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
