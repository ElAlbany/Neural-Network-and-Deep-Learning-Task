{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de0db0a-baa3-42a7-b334-860b6d320c99",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afc84a0-1683-456f-97f5-642624e051b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "     id                                               text     review\n",
      "0  7961  Honestly the best part of this place is the un...  Excellent\n",
      "1  4697  Found Indulge on a whim, based on their huge \"...  Excellent\n",
      "2  4459  My take on Mill street is that it's your class...  Very good\n",
      "3  3714  I think Matt's has had its '5 minutes of fame'...        Bad\n",
      "4  4744  Nobody likes going to the auto body shop..peri...  Excellent\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7000 entries, 0 to 6999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      7000 non-null   int64 \n",
      " 1   text    7000 non-null   object\n",
      " 2   review  7000 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 164.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Step 2: Inspect the first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Check the dataset info\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06c8d9-ee71-4a9f-8488-a3ff22d6c540",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035f2068-639c-4c25-b603-85d6d534bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing some preprocessing\n",
    "import re\n",
    "\n",
    "# convert all comments to lowercase\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# remove {URLs, numbers, punctuation}\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)    # remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove punctuation/numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)     # remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd76a91-06ca-43f8-b71a-af646b9c4917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [honestly, the, best, part, of, this, place, i...\n",
       "1    [found, indulge, on, a, whim, based, on, their...\n",
       "2    [my, take, on, mill, street, is, that, its, yo...\n",
       "3    [i, think, matts, has, had, its, minutes, of, ...\n",
       "4    [nobody, likes, going, to, the, auto, body, sh...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "df['tokens'] = df['text'].str.split()\n",
    "df['tokens'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9744fda4-87e4-4286-9c75-b969093a9569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [honestly, best, part, place, unbelievable, de...\n",
      "1    [found, indulge, whim, based, huge, glutenfree...\n",
      "2    [take, mill, street, classic, collegetown, mai...\n",
      "3    [think, matts, minutes, fame, note, owners, fo...\n",
      "4    [nobody, likes, going, auto, body, shopperiod,...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# stop words removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df['tokens'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d1c10f-f53a-4add-90f1-d710501fd676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [honestly, best, part, place, unbelievable, de...\n",
      "1    [found, indulge, whim, based, huge, glutenfree...\n",
      "2    [take, mill, street, classic, collegetown, mai...\n",
      "3    [think, matt, minute, fame, note, owner, food,...\n",
      "4    [nobody, like, going, auto, body, shopperiod, ...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# applying lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(df['tokens'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c1c175dd-db31-4fd0-b309-b307c3bbf036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping:\n",
      "0 : Bad\n",
      "1 : Excellent\n",
      "2 : Good\n",
      "3 : Very bad\n",
      "4 : Very good\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Encode labels as integers\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['review'])  # 0,1,2,3,4\n",
    "\n",
    "print(\"Class mapping:\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    print(i, \":\", class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a5a486c7-1f8c-49bc-ae22-b4e03a2dbc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>review</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7961</td>\n",
       "      <td>honestly the best part of this place is the un...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>[honestly, best, part, place, unbelievable, de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4697</td>\n",
       "      <td>found indulge on a whim based on their huge gl...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>[found, indulge, whim, based, huge, glutenfree...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4459</td>\n",
       "      <td>my take on mill street is that its your classi...</td>\n",
       "      <td>Very good</td>\n",
       "      <td>[take, mill, street, classic, collegetown, mai...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3714</td>\n",
       "      <td>i think matts has had its minutes of fame note...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>[think, matt, minute, fame, note, owner, food,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4744</td>\n",
       "      <td>nobody likes going to the auto body shopperiod...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>[nobody, like, going, auto, body, shopperiod, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>872</td>\n",
       "      <td>on my way to the airport i decided to stop at ...</td>\n",
       "      <td>Good</td>\n",
       "      <td>[way, airport, decided, stop, ted, bite, eat, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4100</td>\n",
       "      <td>roads are good service not so good this place ...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>[road, good, service, good, place, tourist, tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8995</td>\n",
       "      <td>ive known my way around sky harbor airport sin...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>[ive, known, way, around, sky, harbor, airport...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>657</td>\n",
       "      <td>had lunch here today after hearing all the col...</td>\n",
       "      <td>Good</td>\n",
       "      <td>[lunch, today, hearing, colossal, raf, however...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2942</td>\n",
       "      <td>they dont serve food they serve sex on a plate...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>[dont, serve, food, serve, sex, plate, first, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text     review  \\\n",
       "0  7961  honestly the best part of this place is the un...  Excellent   \n",
       "1  4697  found indulge on a whim based on their huge gl...  Excellent   \n",
       "2  4459  my take on mill street is that its your classi...  Very good   \n",
       "3  3714  i think matts has had its minutes of fame note...        Bad   \n",
       "4  4744  nobody likes going to the auto body shopperiod...  Excellent   \n",
       "5   872  on my way to the airport i decided to stop at ...       Good   \n",
       "6  4100  roads are good service not so good this place ...        Bad   \n",
       "7  8995  ive known my way around sky harbor airport sin...        Bad   \n",
       "8   657  had lunch here today after hearing all the col...       Good   \n",
       "9  2942  they dont serve food they serve sex on a plate...  Excellent   \n",
       "\n",
       "                                              tokens  label  \n",
       "0  [honestly, best, part, place, unbelievable, de...      1  \n",
       "1  [found, indulge, whim, based, huge, glutenfree...      1  \n",
       "2  [take, mill, street, classic, collegetown, mai...      4  \n",
       "3  [think, matt, minute, fame, note, owner, food,...      0  \n",
       "4  [nobody, like, going, auto, body, shopperiod, ...      1  \n",
       "5  [way, airport, decided, stop, ted, bite, eat, ...      2  \n",
       "6  [road, good, service, good, place, tourist, tr...      0  \n",
       "7  [ive, known, way, around, sky, harbor, airport...      0  \n",
       "8  [lunch, today, hearing, colossal, raf, however...      2  \n",
       "9  [dont, serve, food, serve, sex, plate, first, ...      1  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bc2889f3-dc92-4b79-a20d-f4062ce96622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(df['label'], num_classes = 5)  # One-hot encoded labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "226467a8-9f04-4549-8ca4-e90be811b234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a08897eb-ae36-490c-9b49-871cc14bb09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    honestly best part place unbelievable deal get...\n",
      "1    found indulge whim based huge glutenfree menu ...\n",
      "2    take mill street classic collegetown main stri...\n",
      "3    think matt minute fame note owner food good en...\n",
      "4    nobody like going auto body shopperiod guy too...\n",
      "Name: tokens_str, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert list of tokens to space-separated string\n",
    "df['tokens_str'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "texts = df['tokens_str'].values\n",
    "print(df['tokens_str'].head());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ed36b130-b61f-4c79-acee-7a65417dd4f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Word2Vec feature matrix: (7000, 100)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # text vectorization using TF-IDF\n",
    "# vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,1), stop_words='english')\n",
    "# X = vectorizer.fit_transform(texts)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "sentences = df['tokens'].tolist()\n",
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,   # size of embedding vectors\n",
    "    window=5,          # context window\n",
    "    min_count=1,       # ignore words with frequency < 1\n",
    "    workers=4,\n",
    "    sg=1               # skip-gram model\n",
    ")\n",
    "\n",
    "def get_review_vector(tokens, model, vector_size=100):\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# Convert all reviews to vectors\n",
    "X_w2v = np.array([get_review_vector(tokens, w2v_model) for tokens in df['tokens']])\n",
    "print(\"Shape of Word2Vec feature matrix:\", X_w2v.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "020de268-9c7d-4d04-9a28-98b0829745ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = np.array([get_review_vector(tokens, w2v_model) for tokens in df['tokens']])\n",
    "# Test data\n",
    "X_test = np.array([get_review_vector(tokens, w2v_model) for tokens in df['tokens']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "91fc4653-dcb3-47f6-8ce4-4d04c482afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "# Softmax activation\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # stability trick\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def compute_loss(Y, Y_hat):\n",
    "    m = Y.shape[0]\n",
    "    loss = -np.sum(Y * np.log(Y_hat + 1e-9)) / m\n",
    "    return loss\n",
    "\n",
    "# Accuracy\n",
    "def compute_accuracy(Y, Y_hat):\n",
    "    Y_pred = np.argmax(Y_hat, axis=1)\n",
    "    Y_true = np.argmax(Y, axis=1)\n",
    "    return np.mean(Y_pred == Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2517e711-57bb-4714-b9f0-f81a8fb6045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, X):\n",
    "        self.Zs = []\n",
    "        self.As = [X]\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights)-1):\n",
    "            Z = self.As[-1] @ self.weights[i] + self.biases[i]\n",
    "            A = relu(Z)\n",
    "            self.Zs.append(Z)\n",
    "            self.As.append(A)\n",
    "        \n",
    "        # Output layer\n",
    "        Z = self.As[-1] @ self.weights[-1] + self.biases[-1]\n",
    "        A = softmax(Z)\n",
    "        self.Zs.append(Z)\n",
    "        self.As.append(A)\n",
    "        return A\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, Y):\n",
    "        m = Y.shape[0]\n",
    "        grads_W = []\n",
    "        grads_b = []\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dZ = self.As[-1] - Y  # shape: (m, output_size)\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            A_prev = self.As[i]\n",
    "            dW = (A_prev.T @ dZ) / m\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            grads_W.insert(0, dW)\n",
    "            grads_b.insert(0, db)\n",
    "            \n",
    "            if i != 0:\n",
    "                dA_prev = dZ @ self.weights[i].T\n",
    "                dZ = dA_prev * relu_derivative(self.Zs[i-1])\n",
    "        \n",
    "        # Update weights and biases\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads_W[i]\n",
    "            self.biases[i] -= self.learning_rate * grads_b[i]\n",
    "    \n",
    "    # Training function\n",
    "    def train(self, X, Y, epochs=50, batch_size=32, X_val=None, Y_val=None):\n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Shuffle data\n",
    "            idx = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[idx]\n",
    "            Y_shuffled = Y[idx]\n",
    "            \n",
    "            # Mini-batch gradient descent\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                Y_batch = Y_shuffled[i:i+batch_size]\n",
    "                Y_hat = self.forward(X_batch)\n",
    "                self.backward(Y_batch)\n",
    "            \n",
    "            # Compute loss and accuracy for the epoch\n",
    "            Y_hat_train = self.forward(X)\n",
    "            loss = compute_loss(Y, Y_hat_train)\n",
    "            acc = compute_accuracy(Y, Y_hat_train)\n",
    "            \n",
    "            if X_val is not None and Y_val is not None:\n",
    "                Y_hat_val = self.forward(X_val)\n",
    "                val_loss = compute_loss(Y_val, Y_hat_val)\n",
    "                val_acc = compute_accuracy(Y_val, Y_hat_val)\n",
    "                print(f\"Epoch {epoch}: Loss={loss:.4f}, Acc={acc:.4f}, Val_Loss={val_loss:.4f}, Val_Acc={val_acc:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
    "    \n",
    "    # Predict function\n",
    "    def predict(self, X):\n",
    "        Y_hat = self.forward(X)\n",
    "        return np.argmax(Y_hat, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9c58af6c-80ed-40ff-ad97-3395f0fefd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "77cc1f65-02d5-451a-bb85-7060754fad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=1.6067, Acc=0.3538, Val_Loss=1.6068, Val_Acc=0.3486\n",
      "Epoch 2: Loss=1.6040, Acc=0.3538, Val_Loss=1.6041, Val_Acc=0.3486\n",
      "Epoch 3: Loss=1.6014, Acc=0.3538, Val_Loss=1.6015, Val_Acc=0.3486\n",
      "Epoch 4: Loss=1.5988, Acc=0.3538, Val_Loss=1.5990, Val_Acc=0.3486\n",
      "Epoch 5: Loss=1.5963, Acc=0.3538, Val_Loss=1.5965, Val_Acc=0.3486\n",
      "Epoch 6: Loss=1.5938, Acc=0.3538, Val_Loss=1.5940, Val_Acc=0.3486\n",
      "Epoch 7: Loss=1.5913, Acc=0.3538, Val_Loss=1.5916, Val_Acc=0.3486\n",
      "Epoch 8: Loss=1.5888, Acc=0.3538, Val_Loss=1.5892, Val_Acc=0.3486\n",
      "Epoch 9: Loss=1.5864, Acc=0.3538, Val_Loss=1.5868, Val_Acc=0.3486\n",
      "Epoch 10: Loss=1.5841, Acc=0.3538, Val_Loss=1.5845, Val_Acc=0.3486\n",
      "Epoch 11: Loss=1.5818, Acc=0.3538, Val_Loss=1.5822, Val_Acc=0.3486\n",
      "Epoch 12: Loss=1.5795, Acc=0.3538, Val_Loss=1.5800, Val_Acc=0.3486\n",
      "Epoch 13: Loss=1.5772, Acc=0.3538, Val_Loss=1.5778, Val_Acc=0.3486\n",
      "Epoch 14: Loss=1.5750, Acc=0.3538, Val_Loss=1.5756, Val_Acc=0.3486\n",
      "Epoch 15: Loss=1.5728, Acc=0.3538, Val_Loss=1.5734, Val_Acc=0.3486\n",
      "Epoch 16: Loss=1.5706, Acc=0.3538, Val_Loss=1.5713, Val_Acc=0.3486\n",
      "Epoch 17: Loss=1.5685, Acc=0.3538, Val_Loss=1.5692, Val_Acc=0.3486\n",
      "Epoch 18: Loss=1.5664, Acc=0.3538, Val_Loss=1.5672, Val_Acc=0.3486\n",
      "Epoch 19: Loss=1.5644, Acc=0.3538, Val_Loss=1.5652, Val_Acc=0.3486\n",
      "Epoch 20: Loss=1.5624, Acc=0.3538, Val_Loss=1.5632, Val_Acc=0.3486\n",
      "Epoch 21: Loss=1.5604, Acc=0.3538, Val_Loss=1.5612, Val_Acc=0.3486\n",
      "Epoch 22: Loss=1.5584, Acc=0.3538, Val_Loss=1.5593, Val_Acc=0.3486\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m output_size \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# 5 classes\u001b[39;00m\n\u001b[0;32m      5\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(input_size, hidden_sizes, output_size, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m mlp\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, X_val\u001b[38;5;241m=\u001b[39mX_test, Y_val\u001b[38;5;241m=\u001b[39my_test)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[117], line 72\u001b[0m, in \u001b[0;36mMLP.train\u001b[1;34m(self, X, Y, epochs, batch_size, X_val, Y_val)\u001b[0m\n\u001b[0;32m     70\u001b[0m     Y_batch \u001b[38;5;241m=\u001b[39m Y_shuffled[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m     71\u001b[0m     Y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(Y_batch)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Compute loss and accuracy for the epoch\u001b[39;00m\n\u001b[0;32m     75\u001b[0m Y_hat_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X)\n",
      "Cell \u001b[1;32mIn[117], line 57\u001b[0m, in \u001b[0;36mMLP.backward\u001b[1;34m(self, Y)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m grads_W[i]\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m grads_b[i]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]  # number of TF-IDF features\n",
    "hidden_sizes = [512, 256, 128, 64]       # example hidden layers\n",
    "output_size = y_train.shape[1] # 5 classes\n",
    "\n",
    "mlp = MLP(input_size, hidden_sizes, output_size, learning_rate=0.001)\n",
    "mlp.train(X_train, y_train, epochs=200, batch_size=150, X_val=X_test, Y_val=y_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Compute final accuracy\n",
    "accuracy = np.mean(y_pred == np.argmax(y_test, axis=1))\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6e4c7-9c9d-454c-a602-c638f81c549d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
