{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "a9307ebf-67aa-4102-ab4d-4fbb82413115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6c94aed3-29a9-41a5-baa5-5a828333e98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>honestly the best part of this place is the un...</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>found indulge on a whim, based on their huge \"...</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my take on mill street is that it's your class...</td>\n",
       "      <td>Very good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i think matt's has had its '5 minutes of fame'...</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nobody likes going to the auto body shop..peri...</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     review\n",
       "0  honestly the best part of this place is the un...  Excellent\n",
       "1  found indulge on a whim, based on their huge \"...  Excellent\n",
       "2  my take on mill street is that it's your class...  Very good\n",
       "3  i think matt's has had its '5 minutes of fame'...        Bad\n",
       "4  nobody likes going to the auto body shop..peri...  Excellent"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_augmented.csv')\n",
    "data = data.drop('id', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "8bc5c41e-97ee-446d-86ba-fc65f6800b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Labels ----------\n",
      "['Excellent' 'Very good' 'Bad' 'Good' 'Very bad']\n",
      "                                                Text      Label\n",
      "0  honestly the best part of this place is the un...  Excellent\n",
      "1  found indulge on a whim, based on their huge \"...  Excellent\n",
      "2  my take on mill street is that it's your class...  Very good\n",
      "3  i think matt's has had its '5 minutes of fame'...        Bad\n",
      "4  nobody likes going to the auto body shop..peri...  Excellent\n"
     ]
    }
   ],
   "source": [
    "# Modifying the table columns for the loaded data\n",
    "data.columns = ['Text', 'Label']\n",
    "\n",
    "print(\"Sentiment Labels ----------\")\n",
    "print(data.Label.unique())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "42727be7-3d5b-40e7-a296-94b21b116ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after adding new columns ----------\n",
      "                                                Text      Label  Label_Bad  \\\n",
      "0  honestly the best part of this place is the un...  Excellent          0   \n",
      "1  found indulge on a whim, based on their huge \"...  Excellent          0   \n",
      "2  my take on mill street is that it's your class...  Very good          0   \n",
      "3  i think matt's has had its '5 minutes of fame'...        Bad          1   \n",
      "4  nobody likes going to the auto body shop..peri...  Excellent          0   \n",
      "\n",
      "   Label_Excellent  Label_Good  Label_Very bad  Label_Very good  \n",
      "0                1           0               0                0  \n",
      "1                1           0               0                0  \n",
      "2                0           0               0                1  \n",
      "3                0           0               0                0  \n",
      "4                1           0               0                0  \n"
     ]
    }
   ],
   "source": [
    "# apply one hot encoding\n",
    "onehot = pd.get_dummies(data['Label'], prefix='Label')\n",
    "onehot = onehot.astype(int)\n",
    "data = pd.concat([data, onehot], axis=1)\n",
    "\n",
    "print(\"\\nData after adding new columns ----------\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0f4e6dfb-64f9-46ea-836e-5a3c84b0dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# Removing the punctuation marks\n",
    "def remove_punctutations(text):\n",
    "    text_clean = ''\n",
    "    text_clean = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_clean\n",
    "\n",
    "data['Text_Clean'] = data['Text'].apply(lambda x: remove_punctutations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "2ba61c94-027e-4a4d-a64d-03af6b4a72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the words\n",
    "from nltk import word_tokenize\n",
    "tokens = [word_tokenize(sentence) for sentence in data.Text_Clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "0c7fece7-d217-458e-83c9-680e7cbd1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_token(tokens): \n",
    "    return [word.lower() for word in tokens]    \n",
    "\n",
    "# Lowercasing the tokens    \n",
    "lowercased_tokens = [lowercase_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "9cd92a7a-ab59-4a04-99de-3a7115ce826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after removing punctuation marks, stop words and lower casing ----------\n",
      "                                                Text      Label  Label_Bad  \\\n",
      "0  honestly the best part of this place is the un...  Excellent          0   \n",
      "1  found indulge on a whim, based on their huge \"...  Excellent          0   \n",
      "2  my take on mill street is that it's your class...  Very good          0   \n",
      "3  i think matt's has had its '5 minutes of fame'...        Bad          1   \n",
      "4  nobody likes going to the auto body shop..peri...  Excellent          0   \n",
      "\n",
      "   Label_Excellent  Label_Good  Label_Very bad  Label_Very good  \\\n",
      "0                1           0               0                0   \n",
      "1                1           0               0                0   \n",
      "2                0           0               0                1   \n",
      "3                0           0               0                0   \n",
      "4                1           0               0                0   \n",
      "\n",
      "                                          Text_Clean  \\\n",
      "0  honestly the best part of this place is the un...   \n",
      "1  found indulge on a whim based on their huge gl...   \n",
      "2  my take on mill street is that its your classi...   \n",
      "3  i think matts has had its 5 minutes of fame  n...   \n",
      "4  nobody likes going to the auto body shopperiod...   \n",
      "\n",
      "                                          Text_Final  \\\n",
      "0  honestly best part place unbelievable deal get...   \n",
      "1  found indulge whim based huge glutenfree menu ...   \n",
      "2  take mill street classic collegetown main stri...   \n",
      "3  think matts 5 minutes fame note owners food go...   \n",
      "4  nobody likes going auto body shopperiod guys t...   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [honestly, best, part, place, unbelievable, de...  \n",
      "1  [found, indulge, whim, based, huge, glutenfree...  \n",
      "2  [take, mill, street, classic, collegetown, mai...  \n",
      "3  [think, matts, 5, minutes, fame, note, owners,...  \n",
      "4  [nobody, likes, going, auto, body, shopperiod,...  \n"
     ]
    }
   ],
   "source": [
    "# Removing the stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "\n",
    "filtered_words = [remove_stop_words(word) for word in lowercased_tokens]\n",
    "\n",
    "result = [' '.join(word) for word in filtered_words]\n",
    "\n",
    "data['Text_Final'] = result\n",
    "data['Tokens'] = filtered_words\n",
    "#data = data[['Text_Final', 'Tokens', 'Label', 'happiness', 'sadness', 'surprise', 'anger', 'fear']]\n",
    "\n",
    "print(\"\\nData after removing punctuation marks, stop words and lower casing ----------\")\n",
    "print(data.head())\n",
    "\n",
    "#labels = ['Bad', 'Excellent', 'Good', 'Very bad', 'Very good']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "d001be5a-a203-4cab-87a2-caba6f03db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after splitting into Train and Test sets ----------\n",
      "\n",
      "716095 total of Training words with a vocabulary size of 27484\n",
      "Max sentence length is 502\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_data, testing_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nData after splitting into Train and Test sets ----------\\n\")\n",
    "\n",
    "training_words = [word for tokens in training_data[\"Tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in training_data[\"Tokens\"]]\n",
    "training_vocabulary = sorted(list(set(training_words)))\n",
    "print(\"%s total of Training words with a vocabulary size of %s\" % (len(training_words), len(training_vocabulary)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "763bf603-748c-4ae6-86dc-0a3b6b01a51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "176639 total of Testing words with a vocabulary size of 15237\n",
      "Max sentence length is 478\n"
     ]
    }
   ],
   "source": [
    "testing_words = [word for tokens in testing_data[\"Tokens\"] for word in tokens]\n",
    "testing_sentence_lengths = [len(tokens) for tokens in testing_data[\"Tokens\"]]\n",
    "testing_vocabulary = sorted(list(set(testing_words)))\n",
    "print()\n",
    "print(\"%s total of Testing words with a vocabulary size of %s\" % (len(testing_words), len(testing_vocabulary)))\n",
    "print(\"Max sentence length is %s\" % max(testing_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "3c8affdc-1857-4a73-baa9-e4b7f9f7b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training custom Word2Vec ----------\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences=data[\"Tokens\"],   # list of token lists\n",
    "    vector_size=300,            # same as Google News dimension\n",
    "    window=5,\n",
    "    min_count=1,                # keep all words\n",
    "    workers=4\n",
    ")\n",
    "word2vec = word2vec.wv   # keep only the vectors (KeyedVectors)\n",
    "\n",
    "print(\"Completed training custom Word2Vec ----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "236ec3ac-99d4-4fb1-838b-31d34b3aa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting Embeddings\n",
    "def get_average_word2vec(tokens, vector, generate_missing=False, k=300):\n",
    "    if len(tokens)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[token] if token in vector else np.random.rand(k) for token in tokens]\n",
    "    else:\n",
    "        vectorized = [vector[token] if token in vector else np.zeros(k) for token in tokens]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['Tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "training_embeddings = get_word2vec_embeddings(word2vec, training_data, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "89a62c5d-2809-4f21-8b29-fb0242b2fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 27482 unique tokens.\n",
      "(27483, 300)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and Padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(training_vocabulary), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(training_data[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(training_data[\"Text_Final\"].tolist())\n",
    "training_word_index = tokenizer.word_index\n",
    "\n",
    "print('\\nFound %s unique tokens.' % len(training_word_index))\n",
    "\n",
    "training_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# create embedding matrix for CNN\n",
    "train_embedding_weights = np.zeros((len(training_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in training_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_data[\"Text_Final\"].tolist())\n",
    "testing_cnn_data = pad_sequences(testing_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "4a6c90f4-c0e6-4c32-b41a-7c2409f2cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "# Defining the CNN\n",
    "def ConvolutionalNeuralNetwork(embeddings,\n",
    "                               max_sequence_length,\n",
    "                               num_of_words,\n",
    "                               embedding_dim,\n",
    "                               labels_index,\n",
    "                               learning_rate=0.001):\n",
    "    \n",
    "    embedding_layer = Embedding(num_of_words, embedding_dim, weights=[embeddings], input_length=max_sequence_length, trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    sliding_window_heights = [2,3,4,5,6]\n",
    "\n",
    "    for sliding_window_height in sliding_window_heights:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=sliding_window_height, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.5)(l_merge)  \n",
    "   \n",
    "    predictions = Dense(labels_index, activation='softmax', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_ = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = Model(sequence_input, predictions)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "a655f1a5-b126-4dfc-80b3-0736ffdf0a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Text', 'Label', 'Label_Bad', 'Label_Excellent', 'Label_Good',\n",
      "       'Label_Very bad', 'Label_Very good', 'Text_Clean', 'Text_Final',\n",
      "       'Tokens'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626e4b6-bdb9-4b52-b7a2-a001d4799b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the CNN----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,244,900</span> │ input_layer_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">60,100</span> │ embedding_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">90,100</span> │ embedding_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">120,100</span> │ embedding_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,100</span> │ embedding_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">180,100</span> │ embedding_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_100      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_100[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_101      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_101[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_102      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_103      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_103[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_104      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_104[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooling1d_100[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                               │                           │                 │ global_max_pooling1d_101[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                               │                           │                 │ global_max_pooling1d_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                               │                           │                 │ global_max_pooling1d_103[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                               │                           │                 │ global_max_pooling1d_104[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,505</span> │ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_20 (\u001b[38;5;33mEmbedding\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m)           │       \u001b[38;5;34m8,244,900\u001b[0m │ input_layer_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_100 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │          \u001b[38;5;34m60,100\u001b[0m │ embedding_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_101 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │          \u001b[38;5;34m90,100\u001b[0m │ embedding_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_102 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │         \u001b[38;5;34m120,100\u001b[0m │ embedding_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_103 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │         \u001b[38;5;34m150,100\u001b[0m │ embedding_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_104 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │         \u001b[38;5;34m180,100\u001b[0m │ embedding_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_100      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_100[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_101      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_101[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_102      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_102[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_103      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_103[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_104      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_104[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_20 (\u001b[38;5;33mConcatenate\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ global_max_pooling1d_100[\u001b[38;5;34m…\u001b[0m │\n",
       "│                               │                           │                 │ global_max_pooling1d_101[\u001b[38;5;34m…\u001b[0m │\n",
       "│                               │                           │                 │ global_max_pooling1d_102[\u001b[38;5;34m…\u001b[0m │\n",
       "│                               │                           │                 │ global_max_pooling1d_103[\u001b[38;5;34m…\u001b[0m │\n",
       "│                               │                           │                 │ global_max_pooling1d_104[\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ concatenate_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                 │           \u001b[38;5;34m2,505\u001b[0m │ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,847,905</span> (33.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,847,905\u001b[0m (33.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">603,005</span> (2.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m603,005\u001b[0m (2.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,244,900</span> (31.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,244,900\u001b[0m (31.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - acc: 0.3604 - loss: 1.5959 - val_acc: 0.4401 - val_loss: 1.3149\n",
      "Epoch 2/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - acc: 0.4725 - loss: 1.2620 - val_acc: 0.5374 - val_loss: 1.1597\n",
      "Epoch 3/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - acc: 0.5307 - loss: 1.1474 - val_acc: 0.5210 - val_loss: 1.1214\n",
      "Epoch 4/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - acc: 0.5746 - loss: 1.0601 - val_acc: 0.5814 - val_loss: 1.0307\n",
      "Epoch 5/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - acc: 0.6025 - loss: 0.9982 - val_acc: 0.5977 - val_loss: 1.0201\n",
      "Epoch 6/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step - acc: 0.6320 - loss: 0.9236 - val_acc: 0.6018 - val_loss: 0.9823\n",
      "Epoch 7/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - acc: 0.6617 - loss: 0.8598 - val_acc: 0.5977 - val_loss: 1.0020\n",
      "Epoch 8/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step - acc: 0.6743 - loss: 0.8288 - val_acc: 0.6387 - val_loss: 0.9204\n",
      "Epoch 9/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - acc: 0.7006 - loss: 0.7689 - val_acc: 0.6264 - val_loss: 0.9216\n",
      "Epoch 10/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - acc: 0.7160 - loss: 0.7348 - val_acc: 0.6633 - val_loss: 0.8695\n",
      "Epoch 11/100\n",
      "\u001b[1m 11/275\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - acc: 0.7546 - loss: 0.6912"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "# Training the CNN\n",
    "print(\"\\nTraining the CNN----------\")\n",
    "\n",
    "#labels = ['Bad', 'Excellent', 'Good', 'Very bad', 'Very good']\n",
    "labels = ['Label_Bad', 'Label_Excellent', 'Label_Good', 'Label_Very bad', 'Label_Very good']\n",
    "\n",
    "model = ConvolutionalNeuralNetwork(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(training_word_index)+1, EMBEDDING_DIM, \n",
    "               len(list(labels)), learning_rate=0.0008)\n",
    "\n",
    "y_train = training_data[labels].values\n",
    "x_train = training_cnn_data\n",
    "y_tr = y_train\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',verbose=1,\n",
    "    patience = 2,\n",
    "    min_delta = 0.0001,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "  \n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size, callbacks=[es])\n",
    "\n",
    "print(\"\\nCNN trained successfully ----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d957c-4ab7-435d-abc8-e59a1c3fc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Make predictions\n",
    "predictions = model.predict(testing_cnn_data, batch_size=1024, verbose=1)\n",
    "labels = ['Bad', 'Excellent', 'Good', 'Very bad', 'Very good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988239d-dd63-4046-9e27-14650fdf3183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted probabilities to labels\n",
    "prediction_labels = [labels[np.argmax(p)] for p in predictions]\n",
    "\n",
    "# 2. Calculate accuracy\n",
    "testing_data = testing_data.copy()  # avoid modifying original DataFrame\n",
    "testing_data['predicted'] = prediction_labels\n",
    "accuracy = (testing_data['Label'] == testing_data['predicted']).mean() * 100\n",
    "\n",
    "print(f\"Total predictions: {len(testing_data)}\")\n",
    "print(f\"Correct predictions: {(testing_data['Label'] == testing_data['predicted']).sum()}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f367c-4427-4474-9012-4bb8702ae2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the test csv\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# # Assuming you have the same tokenizer you used for training\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "# testing_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# predictions = model.predict(testing_cnn_data, batch_size=32)\n",
    "\n",
    "# # labels = ['Bad', 'Excellent', 'Good', 'Very bad', 'Very good'] \n",
    "# prediction_labels = [labels[np.argmax(p)] for p in predictions]\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     \"id\": test_df['id'],\n",
    "#     \"review\": prediction_labels\n",
    "# })\n",
    "\n",
    "# submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
