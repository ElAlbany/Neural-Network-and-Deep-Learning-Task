{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ff877f-f629-46c2-86a7-864f96327e83",
   "metadata": {},
   "source": [
    "# Data Import & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9307ebf-67aa-4102-ab4d-4fbb82413115",
   "metadata": {
    "id": "a9307ebf-67aa-4102-ab4d-4fbb82413115"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c94aed3-29a9-41a5-baa5-5a828333e98a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6c94aed3-29a9-41a5-baa5-5a828333e98a",
    "outputId": "fa1fc037-70f6-45a7-c6fa-9fe73309ec06"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>honestly the best part of this place is the un...</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>found indulge on a whim, based on their huge \"...</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my take on mill street is that it's your class...</td>\n",
       "      <td>Very good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i think matt's has had its '5 minutes of fame'...</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nobody likes going to the auto body shop..peri...</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     review\n",
       "0  honestly the best part of this place is the un...  Excellent\n",
       "1  found indulge on a whim, based on their huge \"...  Excellent\n",
       "2  my take on mill street is that it's your class...  Very good\n",
       "3  i think matt's has had its '5 minutes of fame'...        Bad\n",
       "4  nobody likes going to the auto body shop..peri...  Excellent"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_augmented.csv')\n",
    "data = data.drop('id', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bc5c41e-97ee-446d-86ba-fc65f6800b96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bc5c41e-97ee-446d-86ba-fc65f6800b96",
    "outputId": "3ef46ae7-f1bd-4193-c999-a2e4dd8dd0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Labels ----------\n",
      "['Excellent' 'Very good' 'Bad' 'Good' 'Very bad']\n",
      "                                                Text      Label\n",
      "0  honestly the best part of this place is the un...  Excellent\n",
      "1  found indulge on a whim, based on their huge \"...  Excellent\n",
      "2  my take on mill street is that it's your class...  Very good\n",
      "3  i think matt's has had its '5 minutes of fame'...        Bad\n",
      "4  nobody likes going to the auto body shop..peri...  Excellent\n"
     ]
    }
   ],
   "source": [
    "# Modifying the table columns for the loaded data\n",
    "data.columns = ['Text', 'Label']\n",
    "\n",
    "print(\"Sentiment Labels ----------\")\n",
    "print(data.Label.unique())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a351b713-682a-4a61-860f-fd0c76877cdb",
   "metadata": {},
   "source": [
    "### Applying One Hot Encoding for output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42727be7-3d5b-40e7-a296-94b21b116ce6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42727be7-3d5b-40e7-a296-94b21b116ce6",
    "outputId": "462947b9-aded-4419-b655-27299ce1aa21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after adding new columns ----------\n",
      "                                                Text      Label  Label_Bad  \\\n",
      "0  honestly the best part of this place is the un...  Excellent          0   \n",
      "1  found indulge on a whim, based on their huge \"...  Excellent          0   \n",
      "2  my take on mill street is that it's your class...  Very good          0   \n",
      "3  i think matt's has had its '5 minutes of fame'...        Bad          1   \n",
      "4  nobody likes going to the auto body shop..peri...  Excellent          0   \n",
      "\n",
      "   Label_Excellent  Label_Good  Label_Very bad  Label_Very good  \n",
      "0                1           0               0                0  \n",
      "1                1           0               0                0  \n",
      "2                0           0               0                1  \n",
      "3                0           0               0                0  \n",
      "4                1           0               0                0  \n"
     ]
    }
   ],
   "source": [
    "# apply one hot encoding\n",
    "onehot = pd.get_dummies(data['Label'], prefix='Label')\n",
    "onehot = onehot.astype(int)\n",
    "data = pd.concat([data, onehot], axis=1)\n",
    "\n",
    "print(\"\\nData after adding new columns ----------\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9dc525-5f87-48fd-b46e-fab5d474a9c0",
   "metadata": {},
   "source": [
    "### Remove punctuation from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f4e6dfb-64f9-46ea-836e-5a3c84b0dea8",
   "metadata": {
    "id": "0f4e6dfb-64f9-46ea-836e-5a3c84b0dea8"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# Removing the punctuation marks\n",
    "def remove_punctutations(text):\n",
    "    text_clean = ''\n",
    "    text_clean = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_clean\n",
    "\n",
    "data['Text_Clean'] = data['Text'].apply(lambda x: remove_punctutations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69453a2e-8b2e-4a9f-929f-0c48ac793ca8",
   "metadata": {},
   "source": [
    "### Text lowecasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ba61c94-027e-4a4d-a64d-03af6b4a72da",
   "metadata": {
    "id": "2ba61c94-027e-4a4d-a64d-03af6b4a72da"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the words\n",
    "from nltk import word_tokenize\n",
    "tokens = [word_tokenize(sentence) for sentence in data.Text_Clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c7fece7-d217-458e-83c9-680e7cbd1072",
   "metadata": {
    "id": "0c7fece7-d217-458e-83c9-680e7cbd1072"
   },
   "outputs": [],
   "source": [
    "def lowercase_token(tokens):\n",
    "    return [word.lower() for word in tokens]\n",
    "\n",
    "# Lowercasing the tokens\n",
    "lowercased_tokens = [lowercase_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2dc32e-05a6-4b9c-b71d-a0f211040a38",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cd92a7a-ab59-4a04-99de-3a7115ce826e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cd92a7a-ab59-4a04-99de-3a7115ce826e",
    "outputId": "1698b472-7f2f-4625-8be8-3361baeec7cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after removing punctuation marks, stop words and lower casing ----------\n",
      "                                                Text      Label  Label_Bad  \\\n",
      "0  honestly the best part of this place is the un...  Excellent          0   \n",
      "1  found indulge on a whim, based on their huge \"...  Excellent          0   \n",
      "2  my take on mill street is that it's your class...  Very good          0   \n",
      "3  i think matt's has had its '5 minutes of fame'...        Bad          1   \n",
      "4  nobody likes going to the auto body shop..peri...  Excellent          0   \n",
      "\n",
      "   Label_Excellent  Label_Good  Label_Very bad  Label_Very good  \\\n",
      "0                1           0               0                0   \n",
      "1                1           0               0                0   \n",
      "2                0           0               0                1   \n",
      "3                0           0               0                0   \n",
      "4                1           0               0                0   \n",
      "\n",
      "                                          Text_Clean  \\\n",
      "0  honestly the best part of this place is the un...   \n",
      "1  found indulge on a whim based on their huge gl...   \n",
      "2  my take on mill street is that its your classi...   \n",
      "3  i think matts has had its 5 minutes of fame  n...   \n",
      "4  nobody likes going to the auto body shopperiod...   \n",
      "\n",
      "                                          Text_Final  \\\n",
      "0  honestly best part place unbelievable deal get...   \n",
      "1  found indulge whim based huge glutenfree menu ...   \n",
      "2  take mill street classic collegetown main stri...   \n",
      "3  think matts 5 minutes fame note owners food go...   \n",
      "4  nobody likes going auto body shopperiod guys t...   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [honestly, best, part, place, unbelievable, de...  \n",
      "1  [found, indulge, whim, based, huge, glutenfree...  \n",
      "2  [take, mill, street, classic, collegetown, mai...  \n",
      "3  [think, matts, 5, minutes, fame, note, owners,...  \n",
      "4  [nobody, likes, going, auto, body, shopperiod,...  \n"
     ]
    }
   ],
   "source": [
    "# Removing the stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "\n",
    "filtered_words = [remove_stop_words(word) for word in lowercased_tokens]\n",
    "\n",
    "result = [' '.join(word) for word in filtered_words]\n",
    "\n",
    "data['Text_Final'] = result\n",
    "data['Tokens'] = filtered_words\n",
    "#data = data[['Text_Final', 'Tokens', 'Label', 'happiness', 'sadness', 'surprise', 'anger', 'fear']]\n",
    "\n",
    "print(\"\\nData after removing punctuation marks, stop words and lower casing ----------\")\n",
    "print(data.head())\n",
    "\n",
    "#labels = ['Bad', 'Excellent', 'Good', 'Very bad', 'Very good']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f84a57-57e4-4141-bd03-24fb70c8bcb3",
   "metadata": {},
   "source": [
    "### Data split into Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d001be5a-a203-4cab-87a2-caba6f03db35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d001be5a-a203-4cab-87a2-caba6f03db35",
    "outputId": "89927691-0982-4abe-9f15-cbcedcc4688d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after splitting into Train and Test sets ----------\n",
      "\n",
      "716095 total of Training words with a vocabulary size of 27484\n",
      "Max sentence length is 502\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_data, testing_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nData after splitting into Train and Test sets ----------\\n\")\n",
    "\n",
    "training_words = [word for tokens in training_data[\"Tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in training_data[\"Tokens\"]]\n",
    "training_vocabulary = sorted(list(set(training_words)))\n",
    "print(\"%s total of Training words with a vocabulary size of %s\" % (len(training_words), len(training_vocabulary)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "763bf603-748c-4ae6-86dc-0a3b6b01a51d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "763bf603-748c-4ae6-86dc-0a3b6b01a51d",
    "outputId": "a7770a4a-4316-45dc-b985-8419c44d5f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "176639 total of Testing words with a vocabulary size of 15237\n",
      "Max sentence length is 478\n"
     ]
    }
   ],
   "source": [
    "testing_words = [word for tokens in testing_data[\"Tokens\"] for word in tokens]\n",
    "testing_sentence_lengths = [len(tokens) for tokens in testing_data[\"Tokens\"]]\n",
    "testing_vocabulary = sorted(list(set(testing_words)))\n",
    "print()\n",
    "print(\"%s total of Testing words with a vocabulary size of %s\" % (len(testing_words), len(testing_vocabulary)))\n",
    "print(\"Max sentence length is %s\" % max(testing_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7795df-be3a-4179-b25e-c6cd6c93926b",
   "metadata": {},
   "source": [
    "### Applying Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c8affdc-1857-4a73-baa9-e4b7f9f7b22a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c8affdc-1857-4a73-baa9-e4b7f9f7b22a",
    "outputId": "ea876912-3cd9-4201-865a-fe44a2e9e100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training custom Word2Vec ----------\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences=data[\"Tokens\"],   # list of token lists\n",
    "    vector_size=300,            \n",
    "    window=5,\n",
    "    min_count=1,             \n",
    "    workers=4\n",
    ")\n",
    "word2vec = word2vec.wv  \n",
    "\n",
    "print(\"Completed training custom Word2Vec ----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "236ec3ac-99d4-4fb1-838b-31d34b3aa269",
   "metadata": {
    "id": "236ec3ac-99d4-4fb1-838b-31d34b3aa269"
   },
   "outputs": [],
   "source": [
    "# Getting Embeddings\n",
    "def get_average_word2vec(tokens, vector, generate_missing=False, k=300):\n",
    "    if len(tokens)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[token] if token in vector else np.random.rand(k) for token in tokens]\n",
    "    else:\n",
    "        vectorized = [vector[token] if token in vector else np.zeros(k) for token in tokens]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['Tokens'].apply(lambda x: get_average_word2vec(x, vectors,\n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "training_embeddings = get_word2vec_embeddings(word2vec, training_data, generate_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836df2c6-3706-4053-8335-5c14647ecef1",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89a62c5d-2809-4f21-8b29-fb0242b2fe5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89a62c5d-2809-4f21-8b29-fb0242b2fe5d",
    "outputId": "c6c64a49-bf55-4fa6-9eaa-1da71bb9d517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 27482 unique tokens.\n",
      "(27483, 300)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and Padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(training_vocabulary), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(training_data[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(training_data[\"Text_Final\"].tolist())\n",
    "training_word_index = tokenizer.word_index\n",
    "\n",
    "print('\\nFound %s unique tokens.' % len(training_word_index))\n",
    "\n",
    "training_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# create embedding matrix for CNN\n",
    "train_embedding_weights = np.zeros((len(training_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in training_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_data[\"Text_Final\"].tolist())\n",
    "testing_cnn_data = pad_sequences(testing_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f0a70-e317-4f46-aeb6-7c99d412ac3f",
   "metadata": {},
   "source": [
    "## Defining CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a6c90f4-c0e6-4c32-b41a-7c2409f2cd06",
   "metadata": {
    "id": "4a6c90f4-c0e6-4c32-b41a-7c2409f2cd06"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "# Defining the CNN\n",
    "def ConvolutionalNeuralNetwork(embeddings,\n",
    "                               max_sequence_length,\n",
    "                               num_of_words,\n",
    "                               embedding_dim,\n",
    "                               labels_index,\n",
    "                               learning_rate=0.001,\n",
    "                               drop = 0.5):\n",
    "\n",
    "    embedding_layer = Embedding(num_of_words, embedding_dim, weights=[embeddings], input_length=max_sequence_length, trainable=False)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    sliding_window_heights = [2,3,4,5,6]\n",
    "\n",
    "    for sliding_window_height in sliding_window_heights:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=sliding_window_height, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(drop)(l_merge)\n",
    "\n",
    "    predictions = Dense(labels_index, activation='softmax', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_ = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = Model(sequence_input, predictions)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a655f1a5-b126-4dfc-80b3-0736ffdf0a29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a655f1a5-b126-4dfc-80b3-0736ffdf0a29",
    "outputId": "cf570559-7b6c-4096-b426-74db725653cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Text', 'Label', 'Label_Bad', 'Label_Excellent', 'Label_Good',\n",
      "       'Label_Very bad', 'Label_Very good', 'Text_Clean', 'Text_Final',\n",
      "       'Tokens'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57118bb1-657b-4ecd-9151-83a82cfb7e7d",
   "metadata": {},
   "source": [
    "# Grid Search for best parameters (LR, Drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be719973-9b9e-461a-b867-a8385c69403e",
   "metadata": {
    "id": "be719973-9b9e-461a-b867-a8385c69403e"
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# import keras\n",
    "# from keras.layers import Dense, Dropout, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# import numpy as np\n",
    "\n",
    "# # Training the CNN\n",
    "# print(\"\\nTraining the CNN----------\")\n",
    "\n",
    "# # Define parameter grid\n",
    "# learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.00001]\n",
    "# dropout_rates = [0.2,0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "# labels = ['Label_Bad', 'Label_Excellent', 'Label_Good', 'Label_Very bad', 'Label_Very good']\n",
    "\n",
    "# y_train = training_data[labels].values\n",
    "# x_train = training_cnn_data\n",
    "# y_tr = y_train\n",
    "\n",
    "# num_epochs = 100\n",
    "# batch_size = 32\n",
    "\n",
    "# best_accuracy = 0\n",
    "# best_params = {}\n",
    "# best_model = None\n",
    "# results = []\n",
    "\n",
    "# # Grid search over parameters\n",
    "# for lr in learning_rates:\n",
    "#     for drop_rate in dropout_rates:\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Testing: Learning Rate={lr}, Dropout Rate={drop_rate}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "\n",
    "#         # Create model with current parameters\n",
    "#         model = ConvolutionalNeuralNetwork(\n",
    "#             train_embedding_weights,\n",
    "#             MAX_SEQUENCE_LENGTH,\n",
    "#             len(training_word_index)+1,\n",
    "#             EMBEDDING_DIM,\n",
    "#             len(list(labels)),\n",
    "#             learning_rate=lr,\n",
    "#             drop=drop_rate\n",
    "#         )\n",
    "\n",
    "#         # Early stopping callback\n",
    "#         es = EarlyStopping(\n",
    "#             monitor='val_loss',\n",
    "#             mode='min',\n",
    "#             verbose=0,  # Set to 0 to reduce verbosity during grid search\n",
    "#             patience=2,\n",
    "#             min_delta=0.0001,\n",
    "#             restore_best_weights=True\n",
    "#         )\n",
    "\n",
    "#         # Train model\n",
    "#         history = model.fit(\n",
    "#             x_train,\n",
    "#             y_tr,\n",
    "#             epochs=num_epochs,\n",
    "#             validation_split=0.1,\n",
    "#             shuffle=True,\n",
    "#             batch_size=batch_size,\n",
    "#             callbacks=[es],\n",
    "#             verbose=1  # Set to 0 to reduce training output\n",
    "#         )\n",
    "\n",
    "#         # Get best validation accuracy\n",
    "#         best_val_acc = max(history.history['val_acc'])\n",
    "#         best_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "#         # Store results\n",
    "#         results.append({\n",
    "#             'learning_rate': lr,\n",
    "#             'dropout_rate': drop_rate,\n",
    "#             'best_val_accuracy': best_val_acc,\n",
    "#             'best_val_loss': best_val_loss,\n",
    "#             'epochs_trained': len(history.history['val_loss']),\n",
    "#             'history': history.history\n",
    "#         })\n",
    "\n",
    "#         print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "#         print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "#         print(f\"Epochs trained: {len(history.history['val_loss'])}\")\n",
    "\n",
    "#         # Update best model if this one is better\n",
    "#         if best_val_acc > best_accuracy:\n",
    "#             best_accuracy = best_val_acc\n",
    "#             best_params = {'learning_rate': lr, 'dropout_rate': drop_rate}\n",
    "#             best_model = model\n",
    "#             print(f\"*** New best model! ***\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"GRID SEARCH COMPLETE\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Print summary of results\n",
    "# print(\"\\nResults Summary:\")\n",
    "# print(\"-\"*40)\n",
    "# for i, result in enumerate(results):\n",
    "#     print(f\"Config {i+1}: LR={result['learning_rate']:.4f}, Drop={result['dropout_rate']:.1f}, \"\n",
    "#           f\"Val_Acc={result['best_val_accuracy']:.4f}, Val_Loss={result['best_val_loss']:.4f}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(f\"BEST CONFIGURATION:\")\n",
    "# print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "# print(f\"Dropout Rate: {best_params['dropout_rate']}\")\n",
    "# print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Save or use the best model\n",
    "# print(\"\\nBest model is ready for use!\")\n",
    "\n",
    "# # If you want to retrain the best model on full data or make predictions:\n",
    "# # final_model = ConvolutionalNeuralNetwork(\n",
    "# #     train_embedding_weights,\n",
    "# #     MAX_SEQUENCE_LENGTH,\n",
    "# #     len(training_word_index)+1,\n",
    "# #     EMBEDDING_DIM,\n",
    "# #     len(list(labels)),\n",
    "# #     learning_rate=best_params['learning_rate'],\n",
    "# #     drop=best_params['dropout_rate']\n",
    "# # )\n",
    "# #\n",
    "# # # Train on full data without validation split\n",
    "# # final_model.fit(x_train, y_tr, epochs=num_epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e6220-90fc-4abe-ac2a-64ce7b4e24fc",
   "metadata": {},
   "source": [
    "# Train the CNN with the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f626e4b6-bdb9-4b52-b7a2-a001d4799b63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f626e4b6-bdb9-4b52-b7a2-a001d4799b63",
    "outputId": "fb1caf84-c4ac-4e43-9181-1e0c5a10f9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the CNN----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,244,900</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">60,100</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">90,100</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">120,100</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,100</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">180,100</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_20       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_21       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_22       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_23       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_24       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooling1d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                               │                           │                 │ global_max_pooling1d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                               │                           │                 │ global_max_pooling1d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                               │                           │                 │ global_max_pooling1d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                               │                           │                 │ global_max_pooling1d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,505</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m)           │       \u001b[38;5;34m8,244,900\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_20 (\u001b[38;5;33mConv1D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │          \u001b[38;5;34m60,100\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_21 (\u001b[38;5;33mConv1D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │          \u001b[38;5;34m90,100\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_22 (\u001b[38;5;33mConv1D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │         \u001b[38;5;34m120,100\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_23 (\u001b[38;5;33mConv1D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │         \u001b[38;5;34m150,100\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d_24 (\u001b[38;5;33mConv1D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │         \u001b[38;5;34m180,100\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_20       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_21       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_22       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_23       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d_24       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ conv1d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_4 (\u001b[38;5;33mConcatenate\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ global_max_pooling1d_20[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                               │                           │                 │ global_max_pooling1d_21[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                               │                           │                 │ global_max_pooling1d_22[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                               │                           │                 │ global_max_pooling1d_23[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                               │                           │                 │ global_max_pooling1d_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                 │           \u001b[38;5;34m2,505\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,847,905</span> (33.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,847,905\u001b[0m (33.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">603,005</span> (2.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m603,005\u001b[0m (2.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,244,900</span> (31.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,244,900\u001b[0m (31.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - acc: 0.2670 - loss: 1.8720 - val_acc: 0.4350 - val_loss: 1.3822\n",
      "Epoch 2/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - acc: 0.3877 - loss: 1.4585 - val_acc: 0.4862 - val_loss: 1.2958\n",
      "Epoch 3/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.4555 - loss: 1.2962 - val_acc: 0.4759 - val_loss: 1.2767\n",
      "Epoch 4/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - acc: 0.5045 - loss: 1.2261 - val_acc: 0.4923 - val_loss: 1.2261\n",
      "Epoch 5/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.5381 - loss: 1.1587 - val_acc: 0.5169 - val_loss: 1.1905\n",
      "Epoch 6/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.5613 - loss: 1.1168 - val_acc: 0.5159 - val_loss: 1.1757\n",
      "Epoch 7/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.5924 - loss: 1.0619 - val_acc: 0.5455 - val_loss: 1.1378\n",
      "Epoch 8/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.6146 - loss: 1.0236 - val_acc: 0.5629 - val_loss: 1.1209\n",
      "Epoch 9/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.6283 - loss: 1.0000 - val_acc: 0.5752 - val_loss: 1.1068\n",
      "Epoch 10/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.6479 - loss: 0.9551 - val_acc: 0.5844 - val_loss: 1.0722\n",
      "Epoch 11/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - acc: 0.6599 - loss: 0.9291 - val_acc: 0.5773 - val_loss: 1.0856\n",
      "Epoch 12/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - acc: 0.6739 - loss: 0.9033 - val_acc: 0.6059 - val_loss: 1.0395\n",
      "Epoch 13/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.6901 - loss: 0.8696 - val_acc: 0.6018 - val_loss: 1.0192\n",
      "Epoch 14/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.6989 - loss: 0.8413 - val_acc: 0.6070 - val_loss: 1.0223\n",
      "Epoch 15/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.7029 - loss: 0.8309 - val_acc: 0.6131 - val_loss: 1.0016\n",
      "Epoch 16/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.7179 - loss: 0.7992 - val_acc: 0.6254 - val_loss: 0.9769\n",
      "Epoch 17/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.7297 - loss: 0.7779 - val_acc: 0.6295 - val_loss: 0.9771\n",
      "Epoch 18/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.7391 - loss: 0.7517 - val_acc: 0.6346 - val_loss: 0.9615\n",
      "Epoch 19/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.7365 - loss: 0.7493 - val_acc: 0.6336 - val_loss: 0.9482\n",
      "Epoch 20/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - acc: 0.7518 - loss: 0.7159 - val_acc: 0.6377 - val_loss: 0.9420\n",
      "Epoch 21/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.7595 - loss: 0.7036 - val_acc: 0.6428 - val_loss: 0.9281\n",
      "Epoch 22/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.7670 - loss: 0.6796 - val_acc: 0.6448 - val_loss: 0.9221\n",
      "Epoch 23/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.7802 - loss: 0.6607 - val_acc: 0.6489 - val_loss: 0.9170\n",
      "Epoch 24/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.7824 - loss: 0.6488 - val_acc: 0.6602 - val_loss: 0.9096\n",
      "Epoch 25/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.7867 - loss: 0.6367 - val_acc: 0.6530 - val_loss: 0.9055\n",
      "Epoch 26/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.7914 - loss: 0.6254 - val_acc: 0.6622 - val_loss: 0.8934\n",
      "Epoch 27/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - acc: 0.7963 - loss: 0.6133 - val_acc: 0.6673 - val_loss: 0.8840\n",
      "Epoch 28/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.8008 - loss: 0.5995 - val_acc: 0.6612 - val_loss: 0.8885\n",
      "Epoch 29/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.8054 - loss: 0.5894 - val_acc: 0.6581 - val_loss: 0.8760\n",
      "Epoch 30/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.8032 - loss: 0.5835 - val_acc: 0.6694 - val_loss: 0.8709\n",
      "Epoch 31/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.8203 - loss: 0.5618 - val_acc: 0.6602 - val_loss: 0.8775\n",
      "Epoch 32/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - acc: 0.8219 - loss: 0.5530 - val_acc: 0.6673 - val_loss: 0.8666\n",
      "Epoch 33/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.8235 - loss: 0.5378 - val_acc: 0.6673 - val_loss: 0.8620\n",
      "Epoch 34/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.8336 - loss: 0.5258 - val_acc: 0.6592 - val_loss: 0.8531\n",
      "Epoch 35/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.8346 - loss: 0.5218 - val_acc: 0.6633 - val_loss: 0.8573\n",
      "Epoch 36/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - acc: 0.8353 - loss: 0.5143 - val_acc: 0.6817 - val_loss: 0.8446\n",
      "Epoch 37/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.8386 - loss: 0.5036 - val_acc: 0.6673 - val_loss: 0.8615\n",
      "Epoch 38/100\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - acc: 0.8403 - loss: 0.4937 - val_acc: 0.6694 - val_loss: 0.8513\n",
      "Epoch 38: early stopping\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "\n",
      "CNN trained successfully ----------\n",
      "\n",
      "Training Summary:\n",
      "   Total epochs run: 38\n",
      "   Best epoch: 36\n",
      "   Best validation loss: 0.8446\n",
      "   Final validation loss: 0.8513\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "# Training the CNN\n",
    "print(\"\\nTraining the CNN----------\")\n",
    "\n",
    "#labels = ['Bad', 'Excellent', 'Good', 'Very bad', 'Very good']\n",
    "labels = ['Label_Bad', 'Label_Excellent', 'Label_Good', 'Label_Very bad', 'Label_Very good']\n",
    "\n",
    "model = ConvolutionalNeuralNetwork(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(training_word_index)+1, EMBEDDING_DIM,\n",
    "               len(list(labels)), learning_rate=0.0001, drop = 0.5)\n",
    "\n",
    "y_train = training_data[labels].values\n",
    "x_train = training_cnn_data\n",
    "y_tr = y_train\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',verbose=1,\n",
    "    patience = 2,\n",
    "    min_delta = 0.0001,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size, callbacks=[es])\n",
    "\n",
    "print(\"\\nCNN trained successfully ----------\")\n",
    "# Find the epoch with minimum validation loss\n",
    "best_epoch = np.argmin(hist.history['val_loss']) + 1  # +1 for human-readable\n",
    "best_val_loss = min(hist.history['val_loss'])\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"   Total epochs run: {len(hist.history['val_loss'])}\")\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"   Final validation loss: {hist.history['val_loss'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad62d5-c87f-4cf5-96e8-5618825fe17f",
   "metadata": {},
   "source": [
    "# Calculate testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c97d957c-4ab7-435d-abc8-e59a1c3fc8d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c97d957c-4ab7-435d-abc8-e59a1c3fc8d1",
    "outputId": "1984f42d-f538-4d9e-ebd5-320a410cdc6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Make predictions\n",
    "predictions = model.predict(testing_cnn_data, batch_size=1024, verbose=1)\n",
    "labels = ['Bad', 'Excellent', 'Good', 'Very bad', 'Very good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2988239d-dd63-4046-9e27-14650fdf3183",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2988239d-dd63-4046-9e27-14650fdf3183",
    "outputId": "f88ca2d6-8210-4431-9ca1-e6471e949992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 2443\n",
      "Correct predictions: 1634\n",
      "Accuracy: 66.88%\n"
     ]
    }
   ],
   "source": [
    "# Convert predicted probabilities to labels\n",
    "prediction_labels = [labels[np.argmax(p)] for p in predictions]\n",
    "\n",
    "# 2. Calculate accuracy\n",
    "testing_data = testing_data.copy()  # avoid modifying original DataFrame\n",
    "testing_data['predicted'] = prediction_labels\n",
    "accuracy = (testing_data['Label'] == testing_data['predicted']).mean() * 100\n",
    "\n",
    "print(f\"Total predictions: {len(testing_data)}\")\n",
    "print(f\"Correct predictions: {(testing_data['Label'] == testing_data['predicted']).sum()}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194142f-4aa3-4472-a83e-d1dba9f759a3",
   "metadata": {},
   "source": [
    "# Running Kaggle test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a9f367c-4427-4474-9012-4bb8702ae2fa",
   "metadata": {
    "id": "6a9f367c-4427-4474-9012-4bb8702ae2fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing pipeline to test data...\n",
      "Preprocessed 3000 test samples\n",
      "Sample original text: we went back here again this past weekend...actually we went there 3 more times this past weekend al...\n",
      "Sample cleaned text: went back past weekendactually went 3 times past weekend alone couldnt resist awesome service great ...\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the test csv\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "# Applying same preprocessing pipeline\n",
    "print(\"Applying preprocessing pipeline to test data...\")\n",
    "\n",
    "# punctuation removal\n",
    "test_df['Text_Clean'] = test_df['text'].apply(lambda x: remove_punctutations(x))\n",
    "\n",
    "# Tokenize\n",
    "test_tokens = [word_tokenize(sentence) for sentence in test_df['Text_Clean']]\n",
    "\n",
    "# Lowercase \n",
    "test_lowercased_tokens = [lowercase_token(token) for token in test_tokens]\n",
    "\n",
    "# Remove stopwords\n",
    "test_filtered_words = [remove_stop_words(word) for word in test_lowercased_tokens]\n",
    "\n",
    "# Rejoin tokens to string\n",
    "test_result = [' '.join(word) for word in test_filtered_words]\n",
    "test_df['Text_Final'] = test_result\n",
    "test_df['Tokens'] = test_filtered_words\n",
    "\n",
    "print(f\"Preprocessed {len(test_df)} test samples\")\n",
    "print(f\"Sample original text: {test_df['text'].iloc[0][:100]}...\")\n",
    "print(f\"Sample cleaned text: {test_df['Text_Final'].iloc[0][:100]}...\")\n",
    "\n",
    "\n",
    "# using the same tokenizer used for training\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "testing_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "predictions = model.predict(testing_cnn_data, batch_size=32)\n",
    "\n",
    "# labels = ['Bad', 'Excellent', 'Good', 'Very bad', 'Very good']\n",
    "prediction_labels = [labels[np.argmax(p)] for p in predictions]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df['id'],\n",
    "    \"review\": prediction_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6018c52-07fd-4dff-8d0d-fe737b23ef8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
